# è²³ã€è³‡æ–™åˆ†æèˆ‡è¦–è¦ºåŒ–

## ä¸€ã€Pandas è³‡æ–™åˆ†æåŸºç¤

### (ä¸€) Pandas åŸºç¤æ¦‚å¿µ

**ä»€éº¼æ˜¯ Pandasï¼Ÿ**
- ğŸ¼ Pandas æ˜¯ Python ä¸­æœ€é‡è¦çš„è³‡æ–™åˆ†æåº«
- ğŸ“Š ä¸»è¦ç”¨æ–¼è™•ç†çµæ§‹åŒ–è³‡æ–™ï¼ˆå¦‚ Excel è¡¨æ ¼ï¼‰
- ğŸ”„ æä¾›è³‡æ–™æ¸…ç†ã€è½‰æ›ã€åˆ†æçš„å®Œæ•´å·¥å…·
- ğŸ’¡ **æƒ³åƒæˆ**ï¼šæŠŠ Excel çš„åŠŸèƒ½ç”¨ç¨‹å¼ç¢¼å¯¦ç¾

**æ ¸å¿ƒè³‡æ–™çµæ§‹æ¯”è¼ƒ**

| çµæ§‹ | èªªæ˜ | é¡æ¯” | ä½¿ç”¨æ™‚æ©Ÿ |
|------|------|------|----------|
| Series | ä¸€ç¶­è³‡æ–™ï¼ˆå–®ä¸€æ¬„ä½ï¼‰ | Excel çš„ä¸€æ¬„ | è™•ç†å–®ä¸€è®Šæ•¸ |
| DataFrame | äºŒç¶­è³‡æ–™ï¼ˆå¤šæ¬„ä½è¡¨æ ¼ï¼‰ | å®Œæ•´çš„ Excel è¡¨æ ¼ | å¤§éƒ¨åˆ†è³‡æ–™åˆ†æ |

### (äºŒ) å»ºç«‹å’Œè®€å–è³‡æ–™

```python
import pandas as pd
import numpy as np

# 1. æ‰‹å‹•å‰µå»º DataFrame - é©åˆç·´ç¿’å’Œå°å‹è³‡æ–™
df = pd.DataFrame({
    'A': [1, 2, 3],           # æ•¸å€¼æ¬„ä½
    'B': ['a', 'b', 'c']      # æ–‡å­—æ¬„ä½
})
print("æ‰‹å‹•å»ºç«‹çš„ DataFrame:")
print(df)

# 2. å¾æª”æ¡ˆè®€å–è³‡æ–™ - å¯¦éš›å·¥ä½œä¸­æœ€å¸¸ç”¨
df_csv = pd.read_csv('data.csv')        # è®€å– CSV æª”æ¡ˆ
df_excel = pd.read_excel('data.xlsx')   # è®€å– Excel æª”æ¡ˆ

# è®€å–æ™‚å¸¸ç”¨åƒæ•¸
df_csv = pd.read_csv('data.csv', 
                     encoding='utf-8',     # ä¸­æ–‡ç·¨ç¢¼
                     index_col=0,          # ç¬¬ä¸€æ¬„ç•¶ä½œç´¢å¼•
                     na_values=['', 'ç„¡']) # æŒ‡å®šç©ºå€¼çš„è¡¨ç¤ºæ–¹å¼

# ğŸ’¡ å¯¦ç”¨æŠ€å·§ï¼šè®€å–å¤§æª”æ¡ˆæ™‚å…ˆçœ‹å‰å¹¾è¡Œ
df_sample = pd.read_csv('large_file.csv', nrows=5)  # åªè®€å–å‰5è¡Œ
print("æª”æ¡ˆé è¦½:")
print(df_sample)
```

### (ä¸‰) è³‡æ–™æ¢ç´¢ - äº†è§£ä½ çš„è³‡æ–™

```python
# ğŸ“‹ åŸºæœ¬è³‡è¨ŠæŸ¥çœ‹ - ç¬¬ä¸€æ­¥ä¸€å®šè¦åšï¼
df.info()          # ğŸ“Š æŸ¥çœ‹è³‡æ–™æ¡†æ¶æ•´é«”è³‡è¨Šï¼ˆæ¬„ä½é¡å‹ã€è¨˜æ†¶é«”ä½¿ç”¨ï¼‰
df.describe()      # ğŸ“ˆ æ•¸å€¼å‹æ¬„ä½çš„çµ±è¨ˆæ‘˜è¦ï¼ˆå¹³å‡ã€æœ€å¤§æœ€å°å€¼ç­‰ï¼‰
df.head()          # ğŸ‘€ æŸ¥çœ‹å‰5è¡Œï¼ˆé è¨­ï¼‰
df.head(10)        # ğŸ‘€ æŸ¥çœ‹å‰10è¡Œ
df.tail()          # ğŸ‘€ æŸ¥çœ‹æœ€å¾Œ5è¡Œ
df.shape           # ğŸ“ æŸ¥çœ‹ç¶­åº¦ (è¡Œæ•¸, æ¬„æ•¸)

# ğŸ’¡ ç‚ºä»€éº¼è¦å…ˆæ¢ç´¢ï¼Ÿ
# - äº†è§£è³‡æ–™å¤§å°å’Œçµæ§‹
# - ç™¼ç¾ç¼ºå¤±å€¼å’Œç•°å¸¸å€¼
# - ç¢ºèªè³‡æ–™é¡å‹æ˜¯å¦æ­£ç¢º

print(f"è³‡æ–™é›†æœ‰ {df.shape[0]} è¡Œï¼Œ{df.shape[1]} æ¬„")
print(f"æ¬„ä½åç¨±ï¼š{list(df.columns)}")
```

### (å››) è³‡æ–™é¸å–å’Œéæ¿¾

```python
# ğŸ¯ é¸å–è³‡æ–™çš„ä¸åŒæ–¹æ³•

# 1. é¸å–æ¬„ä½
df['A']              # é¸å–å–®ä¸€æ¬„ä½ â†’ å›å‚³ Series
df[['A', 'B']]       # é¸å–å¤šå€‹æ¬„ä½ â†’ å›å‚³ DataFrame
                     # ğŸ’¡ æ³¨æ„ï¼šé›™å±¤ä¸­æ‹¬è™Ÿï¼

# 2. é¸å–è¡Œï¼ˆåˆ—ï¼‰
df.loc[0]            # æŒ‰æ¨™ç±¤é¸å–ç¬¬ä¸€è¡Œ
df.loc[0:2]          # é¸å–ç¬¬0åˆ°ç¬¬2è¡Œ
df.iloc[0]           # æŒ‰ä½ç½®é¸å–ç¬¬ä¸€è¡Œ
df.iloc[0:3]         # é¸å–å‰3è¡Œï¼ˆä¸åŒ…å«ç¬¬3è¡Œï¼‰

# 3. åŒæ™‚é¸å–è¡Œå’Œåˆ—
df.loc[0:2, 'A']          # é¸å–ç¬¬0-2è¡Œçš„Aæ¬„
df.iloc[0:3, 0:2]         # é¸å–å‰3è¡Œã€å‰2æ¬„

# ğŸ” æ¢ä»¶éæ¿¾ - éå¸¸å¯¦ç”¨ï¼
df[df['A'] > 2]           # æ¢ä»¶éæ¿¾ï¼šAæ¬„å¤§æ–¼2çš„æ‰€æœ‰è¡Œ
df[df['B'] == 'a']        # æ–‡å­—æ¢ä»¶ï¼šBæ¬„ç­‰æ–¼'a'çš„è¡Œ
df[(df['A'] > 1) & (df['A'] < 3)]  # è¤‡åˆæ¢ä»¶ï¼šä¸”
df[(df['A'] > 2) | (df['B'] == 'a')]  # è¤‡åˆæ¢ä»¶ï¼šæˆ–

# ğŸ’¡ query æ–¹æ³• - æ›´ç›´è§€çš„å¯«æ³•
df.query('A > 2')         # ç­‰åŒæ–¼ df[df['A'] > 2]
df.query('A > 1 and A < 3')  # è¤‡åˆæ¢ä»¶çš„ç°¡æ½”å¯«æ³•
```

### (äº”) è³‡æ–™æ’åºå’Œçµ±è¨ˆ

```python
# ğŸ“Š æ’åºæ“ä½œ
df.sort_values('A', ascending=False)  # æŒ‰Aæ¬„é™åºæ’åº
df.sort_values(['A', 'B'])           # å¤šæ¬„ä½æ’åºï¼šå…ˆæŒ‰Aï¼Œå†æŒ‰B
df.sort_index()                      # æŒ‰ç´¢å¼•æ’åº

# ğŸ“ˆ åŸºç¤çµ±è¨ˆ - äº†è§£è³‡æ–™åˆ†å¸ƒ
df['A'].mean()      # å¹³å‡å€¼ - äº†è§£è³‡æ–™ä¸­å¿ƒè¶¨å‹¢
df['A'].median()    # ä¸­ä½æ•¸ - ä¸å—æ¥µç«¯å€¼å½±éŸ¿çš„ä¸­å¿ƒå€¼
df['A'].mode()      # çœ¾æ•¸ - æœ€å¸¸å‡ºç¾çš„å€¼
df['A'].std()       # æ¨™æº–å·® - è³‡æ–™åˆ†æ•£ç¨‹åº¦
df['A'].var()       # è®Šç•°æ•¸ - æ¨™æº–å·®çš„å¹³æ–¹
df['A'].min()       # æœ€å°å€¼
df['A'].max()       # æœ€å¤§å€¼
df['A'].count()     # éç©ºå€¼å€‹æ•¸

# ğŸ’¡ å¯¦ç”¨æŠ€å·§ï¼šä¸€æ¬¡çœ‹æ‰€æœ‰çµ±è¨ˆ
df.describe()       # è‡ªå‹•è¨ˆç®—æ‰€æœ‰æ•¸å€¼æ¬„ä½çš„çµ±è¨ˆæ‘˜è¦
```

### (å…­) è³‡æ–™æ¸…ç†èˆ‡é è™•ç†

**ç‚ºä»€éº¼éœ€è¦è³‡æ–™æ¸…ç†ï¼Ÿ**
- ğŸ§¹ çœŸå¯¦ä¸–ç•Œçš„è³‡æ–™é€šå¸¸å¾ˆã€Œé«’ã€ï¼šæœ‰ç¼ºå¤±å€¼ã€é‡è¤‡ã€éŒ¯èª¤
- ğŸ’¡ **80/20 æ³•å‰‡**ï¼šè³‡æ–™ç§‘å­¸å®¶ 80% æ™‚é–“åœ¨æ¸…ç†è³‡æ–™ï¼Œ20% åœ¨åˆ†æ
- ğŸ¯ ç›®æ¨™ï¼šè®“è³‡æ–™è®Šå¾—ã€Œä¹¾æ·¨ã€ï¼Œé©åˆåˆ†æ

#### 1. è™•ç†ç¼ºå¤±å€¼ï¼ˆMissing Valuesï¼‰

```python
# ğŸ” æª¢æŸ¥ç¼ºå¤±å€¼ - å…ˆäº†è§£å•é¡Œæœ‰å¤šåš´é‡
df.isna().sum()           # æ¯æ¬„çš„ç¼ºå¤±å€¼æ•¸é‡
df.isna().sum() / len(df) # æ¯æ¬„çš„ç¼ºå¤±å€¼æ¯”ä¾‹ï¼ˆç™¾åˆ†æ¯”ï¼‰

# è¦–è¦ºåŒ–ç¼ºå¤±å€¼åˆ†å¸ƒ
import seaborn as sns
import matplotlib.pyplot as plt
sns.heatmap(df.isna(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('ç¼ºå¤±å€¼åˆ†å¸ƒåœ–')
plt.show()

# ğŸ’Š è™•ç†ç¼ºå¤±å€¼çš„ç­–ç•¥
# ç­–ç•¥1ï¼šå¡«å……ï¼ˆé©åˆå°‘é‡ç¼ºå¤±ï¼‰
df.fillna(0)                    # ç”¨0å¡«å……æ‰€æœ‰ç¼ºå¤±å€¼
df.fillna(method='ffill')       # ç”¨å‰ä¸€å€‹å€¼å¡«å……
df.fillna(method='bfill')       # ç”¨å¾Œä¸€å€‹å€¼å¡«å……
df['A'].fillna(df['A'].mean())  # ç”¨å¹³å‡å€¼å¡«å……æ•¸å€¼æ¬„ä½
df['B'].fillna(df['B'].mode()[0])  # ç”¨çœ¾æ•¸å¡«å……é¡åˆ¥æ¬„ä½

# ç­–ç•¥2ï¼šåˆªé™¤ï¼ˆé©åˆå¤§é‡ç¼ºå¤±æˆ–ä¸é‡è¦çš„è³‡æ–™ï¼‰
df.dropna()                     # åˆªé™¤åŒ…å«ä»»ä½•ç¼ºå¤±å€¼çš„è¡Œ
df.dropna(subset=['A'])         # åªåœ¨Aæ¬„æœ‰ç¼ºå¤±æ™‚æ‰åˆªé™¤è¡Œ
df.dropna(thresh=2)             # è‡³å°‘è¦æœ‰2å€‹éç©ºå€¼æ‰ä¿ç•™è¡Œ

# ğŸ’¡ é¸æ“‡ç­–ç•¥çš„åŸå‰‡ï¼š
# - ç¼ºå¤± < 5%ï¼šå¯ä»¥åˆªé™¤
# - ç¼ºå¤± 5-15%ï¼šè€ƒæ…®å¡«å……
# - ç¼ºå¤± > 15%ï¼šè¬¹æ…è™•ç†ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ”¶é›†è³‡æ–™
```

#### 2. è™•ç†é‡è¤‡å€¼

```python
# ğŸ” æª¢æŸ¥é‡è¤‡å€¼
df.duplicated().sum()        # é‡è¤‡è¡Œçš„æ•¸é‡
print(f"å…±æœ‰ {df.duplicated().sum()} è¡Œé‡è¤‡è³‡æ–™")

# æŸ¥çœ‹é‡è¤‡çš„è³‡æ–™
duplicate_rows = df[df.duplicated()]
print("é‡è¤‡çš„è³‡æ–™ï¼š")
print(duplicate_rows)

# ğŸ§¹ åˆªé™¤é‡è¤‡å€¼
df_clean = df.drop_duplicates()         # åˆªé™¤é‡è¤‡è¡Œ
df_clean = df.drop_duplicates(subset=['A'])  # åªæ ¹æ“šAæ¬„åˆ¤æ–·æ˜¯å¦é‡è¤‡
df_clean = df.drop_duplicates(keep='last')   # ä¿ç•™æœ€å¾Œä¸€å€‹é‡è¤‡å€¼

print(f"æ¸…ç†å‰ï¼š{len(df)} è¡Œ")
print(f"æ¸…ç†å¾Œï¼š{len(df_clean)} è¡Œ")
print(f"åˆªé™¤äº† {len(df) - len(df_clean)} è¡Œé‡è¤‡è³‡æ–™")
```

#### 3. è³‡æ–™é¡å‹è½‰æ›

```python
# ğŸ”§ ç‚ºä»€éº¼è¦è½‰æ›è³‡æ–™é¡å‹ï¼Ÿ
# - ç¯€çœè¨˜æ†¶é«”ç©ºé–“
# - ç¢ºä¿è¨ˆç®—çš„æ­£ç¢ºæ€§
# - æŸäº›æ“ä½œéœ€è¦ç‰¹å®šé¡å‹

# æª¢æŸ¥ç›®å‰çš„è³‡æ–™é¡å‹
print("ç›®å‰çš„è³‡æ–™é¡å‹ï¼š")
print(df.dtypes)

# ğŸ’± å¸¸è¦‹è½‰æ›
df['A'] = df['A'].astype('int64')      # è½‰æ›ç‚ºæ•´æ•¸
df['B'] = df['B'].astype('category')   # è½‰æ›ç‚ºé¡åˆ¥å‹ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰

# å®‰å…¨è½‰æ›ï¼ˆè™•ç†è½‰æ›å¤±æ•—çš„æƒ…æ³ï¼‰
df['C'] = pd.to_numeric(df['C'], errors='coerce')  # ç„¡æ³•è½‰æ›çš„è®Šæˆ NaN
df['date'] = pd.to_datetime(df['date'], errors='coerce')  # è½‰æ›ç‚ºæ—¥æœŸ

# ğŸ’¡ é¡åˆ¥å‹è³‡æ–™çš„å¥½è™•
# - è¨˜æ†¶é«”ä½¿ç”¨é‡å¤§å¹…æ¸›å°‘ï¼ˆç‰¹åˆ¥æ˜¯é‡è¤‡å€¼å¤šçš„æ¬„ä½ï¼‰
# - è‡ªå‹•æ’åºå’Œç¾¤çµ„åŒ–
df['grade'] = df['grade'].astype('category')
print(f"è½‰æ›å‰è¨˜æ†¶é«”ä½¿ç”¨ï¼š{df.memory_usage().sum()} bytes")
```

#### 4. ç•°å¸¸å€¼æª¢æ¸¬å’Œè™•ç†

```python
# ğŸ¯ ä»€éº¼æ˜¯ç•°å¸¸å€¼ï¼Ÿ
# - èˆ‡å…¶ä»–è§€æ¸¬å€¼æ˜é¡¯ä¸åŒçš„æ•¸æ“šé»
# - å¯èƒ½æ˜¯ï¼šæ¸¬é‡éŒ¯èª¤ã€è³‡æ–™è¼¸å…¥éŒ¯èª¤ã€æˆ–çœŸå¯¦çš„æ¥µç«¯æƒ…æ³

# ğŸ“Š ä½¿ç”¨ IQR æ–¹æ³•æª¢æ¸¬ç•°å¸¸å€¼ï¼ˆæœ€å¸¸ç”¨ï¼‰
def detect_outliers_iqr(df, column):
    """
    ä½¿ç”¨å››åˆ†ä½è·ï¼ˆIQRï¼‰æ–¹æ³•æª¢æ¸¬ç•°å¸¸å€¼
    
    åŸç†ï¼š
    - Q1: ç¬¬25ç™¾åˆ†ä½æ•¸
    - Q3: ç¬¬75ç™¾åˆ†ä½æ•¸  
    - IQR = Q3 - Q1
    - ç•°å¸¸å€¼: < Q1-1.5*IQR æˆ– > Q3+1.5*IQR
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    print(f"{column} æ¬„ä½ç•°å¸¸å€¼æª¢æ¸¬çµæœï¼š")
    print(f"ä¸‹ç•Œï¼š{lower_bound:.2f}, ä¸Šç•Œï¼š{upper_bound:.2f}")
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"ç™¼ç¾ {len(outliers)} å€‹ç•°å¸¸å€¼")
    
    return outliers

# ä½¿ç”¨å‡½æ•¸
outliers = detect_outliers_iqr(df, 'A')

# ğŸ”§ è™•ç†ç•°å¸¸å€¼çš„æ–¹æ³•
def remove_outliers(df, column):
    """ç§»é™¤ç•°å¸¸å€¼"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

def cap_outliers(df, column):
    """é™åˆ¶ç•°å¸¸å€¼ï¼ˆæ¯”ç§»é™¤æ›´ä¿å®ˆï¼‰"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
    return df

# ğŸ’¡ é¸æ“‡è™•ç†æ–¹å¼ï¼š
# - ç§»é™¤ï¼šç¢ºå®šæ˜¯éŒ¯èª¤è³‡æ–™æ™‚
# - é™åˆ¶ï¼šå¯èƒ½æ˜¯çœŸå¯¦æ¥µç«¯å€¼æ™‚
# - ä¿ç•™ï¼šéœ€è¦ç ”ç©¶æ¥µç«¯æƒ…æ³æ™‚
```

#### 5. è³‡æ–™æ¨™æº–åŒ–å’Œæ­£è¦åŒ–

```python
# ğŸ¯ ç‚ºä»€éº¼éœ€è¦æ¨™æº–åŒ–ï¼Ÿ
# - ä¸åŒæ¬„ä½çš„å°ºåº¦å·®ç•°å¾ˆå¤§ï¼ˆå¦‚å¹´é½¡ vs æ”¶å…¥ï¼‰
# - æ©Ÿå™¨å­¸ç¿’ç®—æ³•å°å°ºåº¦æ•æ„Ÿ
# - è®“æ‰€æœ‰ç‰¹å¾µåœ¨åŒä¸€å°ºåº¦ä¸Šæ¯”è¼ƒ

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# æº–å‚™æ•¸æ“šï¼ˆåªå°æ•¸å€¼æ¬„ä½æ¨™æº–åŒ–ï¼‰
numeric_columns = df.select_dtypes(include=[np.number]).columns

# æ–¹æ³•1ï¼šæ¨™æº–åŒ– (Z-score normalization)
# è½‰æ›å¾Œï¼šå¹³å‡å€¼=0ï¼Œæ¨™æº–å·®=1
scaler = StandardScaler()
df_standardized = df.copy()
df_standardized[numeric_columns] = scaler.fit_transform(df[numeric_columns])

print("æ¨™æº–åŒ–å¾Œçš„çµ±è¨ˆï¼š")
print(df_standardized[numeric_columns].describe())

# æ–¹æ³•2ï¼šæœ€å°-æœ€å¤§æ­£è¦åŒ–
# è½‰æ›å¾Œï¼šæ‰€æœ‰å€¼åœ¨ 0-1 ä¹‹é–“
minmax_scaler = MinMaxScaler()
df_normalized = df.copy()
df_normalized[numeric_columns] = minmax_scaler.fit_transform(df[numeric_columns])

# æ–¹æ³•3ï¼šç©©å¥æ¨™æº–åŒ– (é©ç”¨æ–¼æœ‰ç•°å¸¸å€¼çš„è³‡æ–™)
# ä½¿ç”¨ä¸­ä½æ•¸å’Œå››åˆ†ä½è·ï¼Œå°ç•°å¸¸å€¼ä¸æ•æ„Ÿ
robust_scaler = RobustScaler()
df_robust = df.copy()
df_robust[numeric_columns] = robust_scaler.fit_transform(df[numeric_columns])

# ğŸ’¡ é¸æ“‡æ¨™æº–åŒ–æ–¹æ³•çš„æŒ‡å—ï¼š
# - StandardScaler: è³‡æ–™å‘ˆå¸¸æ…‹åˆ†å¸ƒ
# - MinMaxScaler: éœ€è¦å›ºå®šç¯„åœ(0-1)
# - RobustScaler: è³‡æ–™æœ‰ç•°å¸¸å€¼
```

### (ä¸ƒ) è³‡æ–™åˆä½µèˆ‡é‡å¡‘

**ç‚ºä»€éº¼éœ€è¦åˆä½µå’Œé‡å¡‘è³‡æ–™ï¼Ÿ**
- ğŸ”— å¯¦éš›å·¥ä½œä¸­ï¼Œè³‡æ–™é€šå¸¸åˆ†æ•£åœ¨å¤šå€‹æª”æ¡ˆæˆ–è¡¨æ ¼ä¸­
- ğŸ“Š éœ€è¦å°‡ä¸åŒä¾†æºçš„è³‡æ–™æ•´åˆåˆ†æ
- ğŸ”„ ä¸åŒçš„åˆ†æéœ€è¦ä¸åŒçš„è³‡æ–™æ ¼å¼

#### 1. è³‡æ–™åˆä½µï¼ˆConcatenation & Mergingï¼‰

```python
# ğŸ”— å‚ç›´åˆä½µï¼ˆConcatenationï¼‰- æŠŠè¡¨æ ¼ä¸Šä¸‹æ¥èµ·ä¾†
# é©ç”¨ï¼šç›¸åŒæ¬„ä½çš„ä¸åŒè³‡æ–™é›†
df1 = pd.DataFrame({'A': [1, 2], 'B': ['x', 'y']})
df2 = pd.DataFrame({'A': [3, 4], 'B': ['z', 'w']})

df_vertical = pd.concat([df1, df2])              # å‚ç›´åˆä½µ
df_vertical = pd.concat([df1, df2], ignore_index=True)  # é‡æ–°ç·¨è™Ÿç´¢å¼•

print("å‚ç›´åˆä½µçµæœï¼š")
print(df_vertical)

# ğŸ”— æ°´å¹³åˆä½µï¼ˆConcatenationï¼‰- æŠŠè¡¨æ ¼å·¦å³æ¥èµ·ä¾†  
# é©ç”¨ï¼šç›¸åŒè§€æ¸¬å–®ä½çš„ä¸åŒç‰¹å¾µ
df3 = pd.DataFrame({'C': [10, 20], 'D': [30, 40]})
df_horizontal = pd.concat([df1, df3], axis=1)    # axis=1 è¡¨ç¤ºæ°´å¹³åˆä½µ

print("æ°´å¹³åˆä½µçµæœï¼š")
print(df_horizontal)

# ğŸ”— é—œè¯å¼åˆä½µï¼ˆMergeï¼‰- åƒ SQL çš„ JOIN
# æ ¹æ“šå…±åŒæ¬„ä½åˆä½µä¸åŒè¡¨æ ¼

# æº–å‚™ç¤ºä¾‹è³‡æ–™
customers = pd.DataFrame({
    'customer_id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Charlie'],
    'city': ['å°åŒ—', 'é«˜é›„', 'å°ä¸­']
})

orders = pd.DataFrame({
    'order_id': [101, 102, 103, 104],
    'customer_id': [1, 2, 1, 3],
    'amount': [1000, 1500, 800, 2000]
})

# ğŸ’¡ ä¸åŒçš„åˆä½µæ–¹å¼
# Inner Join: åªä¿ç•™å…©é‚Šéƒ½æœ‰çš„è¨˜éŒ„
inner_merge = pd.merge(customers, orders, on='customer_id', how='inner')
print("Inner Join - åªä¿ç•™å…©é‚Šéƒ½æœ‰çš„å®¢æˆ¶ï¼š")
print(inner_merge)

# Left Join: ä¿ç•™å·¦è¡¨æ‰€æœ‰è¨˜éŒ„
left_merge = pd.merge(customers, orders, on='customer_id', how='left')
print("Left Join - ä¿ç•™æ‰€æœ‰å®¢æˆ¶ï¼š")
print(left_merge)

# Right Join: ä¿ç•™å³è¡¨æ‰€æœ‰è¨˜éŒ„  
right_merge = pd.merge(customers, orders, on='customer_id', how='right')

# Outer Join: ä¿ç•™å…©é‚Šæ‰€æœ‰è¨˜éŒ„
outer_merge = pd.merge(customers, orders, on='customer_id', how='outer')

# ğŸ’¡ åˆä½µçš„å¯¦éš›æ‡‰ç”¨å ´æ™¯ï¼š
# - å®¢æˆ¶è³‡æ–™ + è¨‚å–®è³‡æ–™
# - ç”¢å“è³‡æ–™ + éŠ·å”®è³‡æ–™
# - å“¡å·¥è³‡æ–™ + è–ªè³‡è³‡æ–™
```

#### 2. è³‡æ–™é‡å¡‘ï¼ˆReshapeï¼‰

```python
# ğŸ“Š æ¨ç´è¡¨ï¼ˆPivot Tableï¼‰- æŠŠé•·æ ¼å¼è®Šå¯¬æ ¼å¼
# åŸå§‹è³‡æ–™ï¼ˆé•·æ ¼å¼ï¼‰
sales_data = pd.DataFrame({
    'date': ['2023-01', '2023-01', '2023-02', '2023-02'],
    'product': ['A', 'B', 'A', 'B'],
    'sales': [100, 150, 120, 180]
})

print("åŸå§‹éŠ·å”®è³‡æ–™ï¼ˆé•·æ ¼å¼ï¼‰ï¼š")
print(sales_data)

# è½‰æ›ç‚ºæ¨ç´è¡¨ï¼ˆå¯¬æ ¼å¼ï¼‰
pivot_table = sales_data.pivot(index='date', columns='product', values='sales')
print("æ¨ç´è¡¨ï¼ˆå¯¬æ ¼å¼ï¼‰ï¼š")
print(pivot_table)

# ğŸ’¡ æ¨ç´è¡¨çš„ç”¨é€”ï¼š
# - å»ºç«‹äº¤å‰åˆ†æè¡¨
# - è®“è³‡æ–™æ›´æ˜“æ–¼é–±è®€
# - é©åˆè£½ä½œå ±è¡¨

# ğŸ“Š èåŒ–ï¼ˆMeltï¼‰- æŠŠå¯¬æ ¼å¼è®Šé•·æ ¼å¼
# å¯¬æ ¼å¼è³‡æ–™
wide_data = pd.DataFrame({
    'ID': [1, 2, 3],
    'Jan': [100, 200, 150],
    'Feb': [120, 180, 170],
    'Mar': [110, 220, 160]
})

print("å¯¬æ ¼å¼è³‡æ–™ï¼š")
print(wide_data)

# è½‰æ›ç‚ºé•·æ ¼å¼
long_data = wide_data.melt(
    id_vars=['ID'],                    # ä¿æŒä¸è®Šçš„æ¬„ä½
    value_vars=['Jan', 'Feb', 'Mar'],  # è¦èåŒ–çš„æ¬„ä½
    var_name='Month',                  # æ–°çš„è®Šæ•¸åç¨±æ¬„ä½
    value_name='Sales'                 # æ–°çš„æ•¸å€¼æ¬„ä½
)

print("é•·æ ¼å¼è³‡æ–™ï¼š")
print(long_data)

# ğŸ’¡ é•·æ ¼å¼ vs å¯¬æ ¼å¼çš„é¸æ“‡ï¼š
# - é•·æ ¼å¼ï¼šé©åˆçµ±è¨ˆåˆ†æã€è¦–è¦ºåŒ–
# - å¯¬æ ¼å¼ï¼šé©åˆå ±è¡¨ã€äººé¡é–±è®€
```

#### 3. ç¾¤çµ„æ“ä½œï¼ˆGroup Operationsï¼‰

```python
# ğŸ¯ ç¾¤çµ„åˆ†æ - è³‡æ–™åˆ†æçš„æ ¸å¿ƒæŠ€èƒ½
# å¸¸è¦‹å•é¡Œï¼šã€Œæ¯å€‹é¡åˆ¥çš„å¹³å‡å€¼æ˜¯å¤šå°‘ï¼Ÿã€

# æº–å‚™ç¤ºä¾‹è³‡æ–™
employee_data = pd.DataFrame({
    'department': ['IT', 'HR', 'IT', 'HR', 'Finance', 'Finance'],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],
    'salary': [80000, 60000, 85000, 65000, 70000, 75000],
    'experience': [5, 3, 7, 4, 6, 8]
})

# ğŸ“Š åŸºæœ¬ç¾¤çµ„çµ±è¨ˆ
dept_stats = employee_data.groupby('department').mean()
print("å„éƒ¨é–€å¹³å‡è–ªè³‡å’Œç¶“é©—ï¼š")
print(dept_stats)

# ğŸ“Š å¤šé‡çµ±è¨ˆå‡½æ•¸
dept_summary = employee_data.groupby('department').agg({
    'salary': ['mean', 'std', 'min', 'max'],    # è–ªè³‡çš„å¤šé …çµ±è¨ˆ
    'experience': ['mean', 'count']              # ç¶“é©—çš„çµ±è¨ˆ
})
print("å„éƒ¨é–€è©³ç´°çµ±è¨ˆï¼š")
print(dept_summary)

# ğŸ“Š è‡ªå®šç¾©èšåˆå‡½æ•¸
def salary_range(series):
    return series.max() - series.min()

custom_agg = employee_data.groupby('department').agg({
    'salary': ['mean', salary_range],
    'name': 'count'  # è¨ˆç®—äººæ•¸
})

# ğŸ’¡ ç¾¤çµ„æ“ä½œçš„å¯¦éš›æ‡‰ç”¨ï¼š
# - éŠ·å”®åˆ†æï¼šæŒ‰åœ°å€ã€ç”¢å“ã€æ™‚é–“åˆ†çµ„
# - å®¢æˆ¶åˆ†æï¼šæŒ‰å¹´é½¡æ®µã€æ¶ˆè²»ç­‰ç´šåˆ†çµ„  
# - æ•ˆèƒ½åˆ†æï¼šæŒ‰éƒ¨é–€ã€è·ä½åˆ†çµ„

# ğŸ” é€²éšç¾¤çµ„æ“ä½œ
# å¤šæ¬„ä½åˆ†çµ„
multi_group = employee_data.groupby(['department', 'experience > 5']).mean()

# ç¾¤çµ„å¾Œç¯©é¸
high_salary_depts = employee_data.groupby('department').filter(
    lambda x: x['salary'].mean() > 70000
)
print("å¹³å‡è–ªè³‡è¶…é70000çš„éƒ¨é–€å“¡å·¥ï¼š")
print(high_salary_depts)
```

## äºŒã€è³‡æ–™è¦–è¦ºåŒ–

### (ä¸€) è¦–è¦ºåŒ–çš„é‡è¦æ€§

**ç‚ºä»€éº¼éœ€è¦è³‡æ–™è¦–è¦ºåŒ–ï¼Ÿ**
- ğŸ‘ï¸ **å¿«é€Ÿç†è§£**ï¼šä¸€åœ–å‹åƒè¨€ï¼Œå¿«é€Ÿç™¼ç¾æ¨¡å¼å’Œè¶¨å‹¢
- ğŸ§  **ç›´è§€æºé€š**ï¼šå‘éæŠ€è¡“äººå“¡èªªæ˜ç™¼ç¾æ›´å®¹æ˜“
- ğŸ” **ç™¼ç¾æ´å¯Ÿ**ï¼šè¦–è¦ºåŒ–èƒ½æ­éœ²æ•¸å­—è¡¨æ ¼ä¸­çœ‹ä¸å‡ºçš„é—œä¿‚
- ğŸš¨ **è­˜åˆ¥ç•°å¸¸**ï¼šå¿«é€Ÿç™¼ç¾ç•°å¸¸å€¼å’ŒéŒ¯èª¤

**é¸æ“‡åˆé©åœ–è¡¨çš„æŒ‡å—**

| ç›®çš„ | é©åˆçš„åœ–è¡¨ | ä½¿ç”¨æ™‚æ©Ÿ |
|------|----------|----------|
| æ¯”è¼ƒæ•¸å€¼ | é•·æ¢åœ–ã€æŸ±ç‹€åœ– | æ¯”è¼ƒä¸åŒé¡åˆ¥çš„é‡ |
| é¡¯ç¤ºè¶¨å‹¢ | æŠ˜ç·šåœ– | æ™‚é–“åºåˆ—è³‡æ–™ |
| é¡¯ç¤ºåˆ†å¸ƒ | ç›´æ–¹åœ–ã€ç®±å½¢åœ– | äº†è§£è³‡æ–™åˆ†å¸ƒå½¢ç‹€ |
| é¡¯ç¤ºé—œä¿‚ | æ•£é»åœ– | å…©è®Šæ•¸é–“çš„ç›¸é—œæ€§ |
| é¡¯ç¤ºçµ„æˆ | åœ“é¤…åœ–ã€å †ç–Šåœ– | éƒ¨åˆ†å°æ•´é«”çš„æ¯”ä¾‹ |
| é¡¯ç¤ºåœ°ç† | åœ°åœ– | ç©ºé–“åˆ†å¸ƒè³‡æ–™ |

### (äºŒ) Matplotlib åŸºç¤

**ä»€éº¼æ˜¯ Matplotlibï¼Ÿ**
- ğŸ“Š Python æœ€åŸºæœ¬ã€æœ€é‡è¦çš„ç¹ªåœ–åº«
- ğŸ¨ é¡ä¼¼ MATLAB çš„ç¹ªåœ–èªæ³•
- ğŸ”§ é«˜åº¦å¯å®¢è£½åŒ–ï¼Œä½†èªæ³•è¼ƒè¤‡é›œ
- ğŸ—ï¸ å…¶ä»–ç¹ªåœ–åº«çš„åŸºç¤ï¼ˆSeabornã€Pandas plot éƒ½åŸºæ–¼å®ƒï¼‰

```python
import matplotlib.pyplot as plt
import numpy as np

# ğŸ¨ åŸºæœ¬è¨­å®š
plt.rcParams['font.family'] = 'Microsoft YaHei'  # æ”¯æ´ä¸­æ–‡
plt.rcParams['figure.figsize'] = (10, 6)         # é è¨­åœ–ç‰‡å¤§å°

# ğŸ“ˆ åŸºæœ¬ç·šåœ– - é¡¯ç¤ºè¶¨å‹¢
# æº–å‚™è³‡æ–™
x = np.linspace(0, 10, 100)  # 0åˆ°10ä¹‹é–“çš„100å€‹é»
y = np.sin(x)                # æ­£å¼¦å‡½æ•¸

# ç¹ªè£½åœ–è¡¨
plt.figure(figsize=(10, 6))           # è¨­å®šåœ–ç‰‡å¤§å°
plt.plot(x, y, 'r-', label='sin(x)', linewidth=2)  # ç´…è‰²å¯¦ç·š
plt.plot(x, np.cos(x), 'b--', label='cos(x)', linewidth=2)  # è—è‰²è™›ç·š

# ğŸ’¡ ç¾åŒ–åœ–è¡¨
plt.xlabel('Xè»¸æ¨™ç±¤', fontsize=12)    # Xè»¸æ¨™ç±¤
plt.ylabel('Yè»¸æ¨™ç±¤', fontsize=12)    # Yè»¸æ¨™ç±¤
plt.title('ä¸‰è§’å‡½æ•¸åœ–è¡¨', fontsize=14) # åœ–è¡¨æ¨™é¡Œ
plt.legend()                          # é¡¯ç¤ºåœ–ä¾‹
plt.grid(True, alpha=0.3)            # é¡¯ç¤ºç¶²æ ¼ï¼Œé€æ˜åº¦0.3
plt.show()                           # é¡¯ç¤ºåœ–è¡¨

# ğŸ“Š æ•£é»åœ– - é¡¯ç¤ºå…©è®Šæ•¸é—œä¿‚
# ç”Ÿæˆç¤ºä¾‹è³‡æ–™
np.random.seed(42)  # å›ºå®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
x_scatter = np.random.randn(100)
y_scatter = 2 * x_scatter + np.random.randn(100)  # yèˆ‡xæœ‰ç·šæ€§é—œä¿‚åŠ ä¸Šå™ªéŸ³

plt.figure(figsize=(8, 6))
plt.scatter(x_scatter, y_scatter, c='blue', alpha=0.6, s=50)
plt.xlabel('X è®Šæ•¸')
plt.ylabel('Y è®Šæ•¸') 
plt.title('æ•£é»åœ–ï¼šX èˆ‡ Y çš„é—œä¿‚')
plt.grid(True, alpha=0.3)
plt.show()

# ğŸ“Š é•·æ¢åœ– - æ¯”è¼ƒé¡åˆ¥è³‡æ–™
categories = ['ç”¢å“A', 'ç”¢å“B', 'ç”¢å“C', 'ç”¢å“D']
values = [23, 45, 56, 78]

plt.figure(figsize=(8, 6))
bars = plt.bar(categories, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])

# åœ¨é•·æ¢ä¸Šé¡¯ç¤ºæ•¸å€¼
for bar, value in zip(bars, values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
             str(value), ha='center', va='bottom')

plt.xlabel('ç”¢å“é¡åˆ¥')
plt.ylabel('éŠ·å”®é‡')
plt.title('å„ç”¢å“éŠ·å”®é‡æ¯”è¼ƒ')
plt.xticks(rotation=45)  # æ—‹è½‰Xè»¸æ¨™ç±¤
plt.tight_layout()       # è‡ªå‹•èª¿æ•´ä½ˆå±€
plt.show()

# ğŸ“Š å­åœ– - åœ¨ä¸€å€‹åœ–ä¸­é¡¯ç¤ºå¤šå€‹åœ–è¡¨
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))

# å­åœ–1ï¼šç·šåœ–
ax1.plot(x, y)
ax1.set_title('ç·šåœ–')
ax1.grid(True)

# å­åœ–2ï¼šæ•£é»åœ–  
ax2.scatter(x_scatter, y_scatter)
ax2.set_title('æ•£é»åœ–')

# å­åœ–3ï¼šé•·æ¢åœ–
ax3.bar(categories, values)
ax3.set_title('é•·æ¢åœ–')
ax3.tick_params(axis='x', rotation=45)

# å­åœ–4ï¼šç›´æ–¹åœ–
ax4.hist(y_scatter, bins=20, alpha=0.7)
ax4.set_title('ç›´æ–¹åœ–')

plt.tight_layout()  # é¿å…å­åœ–é‡ç–Š
plt.show()

# ğŸ’¡ Matplotlib å°æŠ€å·§ï¼š
# - ä½¿ç”¨ plt.style.use('seaborn') å¿«é€Ÿç¾åŒ–
# - ç”¨ plt.savefig('filename.png', dpi=300) å„²å­˜é«˜ç•«è³ªåœ–ç‰‡
# - ç”¨ plt.close() é‡‹æ”¾è¨˜æ†¶é«”
```

### (ä¸‰) Seaborn è¦–è¦ºåŒ–

**ä»€éº¼æ˜¯ Seabornï¼Ÿ**
- ğŸ¨ åŸºæ–¼ Matplotlib çš„é«˜ç´šçµ±è¨ˆè¦–è¦ºåŒ–åº«
- ğŸ“Š å°ˆé–€è¨­è¨ˆç”¨æ–¼çµ±è¨ˆåœ–è¡¨
- ğŸŒˆ é è¨­å°±å¾ˆç¾è§€ï¼Œè‰²å½©æ­é…å°ˆæ¥­
- ğŸ“ˆ ç‰¹åˆ¥é©åˆæ¢ç´¢æ€§è³‡æ–™åˆ†æï¼ˆEDAï¼‰

```python
import seaborn as sns
import pandas as pd
import numpy as np

# ğŸ¨ è¨­å®š Seaborn é¢¨æ ¼
sns.set_style("whitegrid")        # ç™½è‰²èƒŒæ™¯ + ç¶²æ ¼
sns.set_palette("husl")           # è¨­å®šèª¿è‰²ç›¤
sns.set_context("notebook")       # è¨­å®šå¤§å°ï¼ˆtalk, paper, posterï¼‰

# ğŸ“Š æº–å‚™ç¤ºä¾‹è³‡æ–™
np.random.seed(42)
tips = sns.load_dataset("tips")   # Seaborn å…§å»ºè³‡æ–™é›†
print("Tips è³‡æ–™é›†å‰5è¡Œï¼š")
print(tips.head())

# ğŸ“ˆ åˆ†å¸ƒåœ– - äº†è§£å–®ä¸€è®Šæ•¸çš„åˆ†å¸ƒ
plt.figure(figsize=(12, 4))

# 1. ç›´æ–¹åœ– + å¯†åº¦æ›²ç·š
plt.subplot(1, 3, 1)
sns.histplot(tips['total_bill'], kde=True)
plt.title('ç¸½å¸³å–®åˆ†å¸ƒ')

# 2. åªæœ‰å¯†åº¦æ›²ç·š
plt.subplot(1, 3, 2)
sns.kdeplot(tips['total_bill'])
plt.title('ç¸½å¸³å–®å¯†åº¦åœ–')

# 3. å¤šçµ„æ¯”è¼ƒ
plt.subplot(1, 3, 3)
sns.histplot(data=tips, x='total_bill', hue='time', alpha=0.7)
plt.title('æŒ‰æ™‚é–“åˆ†çµ„çš„ç¸½å¸³å–®åˆ†å¸ƒ')

plt.tight_layout()
plt.show()

# ğŸ“Š ç®±å½¢åœ– - æ¯”è¼ƒä¸åŒç¾¤çµ„çš„åˆ†å¸ƒ
plt.figure(figsize=(12, 4))

# 1. åŸºæœ¬ç®±å½¢åœ–
plt.subplot(1, 3, 1)
sns.boxplot(data=tips, x='day', y='total_bill')
plt.title('å„å¤©ç¸½å¸³å–®åˆ†å¸ƒ')
plt.xticks(rotation=45)

# 2. åˆ†çµ„ç®±å½¢åœ–
plt.subplot(1, 3, 2)
sns.boxplot(data=tips, x='day', y='total_bill', hue='time')
plt.title('æŒ‰æ™‚é–“åˆ†çµ„çš„ç¸½å¸³å–®')
plt.xticks(rotation=45)

# 3. å°æç´åœ– - çµåˆç®±å½¢åœ–å’Œå¯†åº¦åœ–
plt.subplot(1, 3, 3)
sns.violinplot(data=tips, x='day', y='total_bill')
plt.title('å°æç´åœ–')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# ğŸ“Š é—œä¿‚åœ– - æ¢ç´¢è®Šæ•¸é–“çš„é—œä¿‚
plt.figure(figsize=(15, 5))

# 1. æ•£é»åœ–
plt.subplot(1, 3, 1)
sns.scatterplot(data=tips, x='total_bill', y='tip', hue='time', size='size')
plt.title('ç¸½å¸³å–® vs å°è²»ï¼ˆæŒ‰æ™‚é–“å’Œäººæ•¸åˆ†çµ„ï¼‰')

# 2. å›æ­¸ç·šåœ–
plt.subplot(1, 3, 2)
sns.regplot(data=tips, x='total_bill', y='tip', scatter_kws={'alpha':0.6})
plt.title('ç¸½å¸³å–® vs å°è²»ï¼ˆå«å›æ­¸ç·šï¼‰')

# 3. åˆ†çµ„å›æ­¸ç·š
plt.subplot(1, 3, 3)
sns.lmplot(data=tips, x='total_bill', y='tip', hue='smoker', height=4)
plt.title('æŒ‰æ˜¯å¦å¸è¸åˆ†çµ„çš„å›æ­¸')

plt.tight_layout()
plt.show()

# ğŸ”¥ ç†±åŠ›åœ– - é¡¯ç¤ºç›¸é—œæ€§çŸ©é™£
# è¨ˆç®—ç›¸é—œæ€§çŸ©é™£
correlation_matrix = tips.select_dtypes(include=[np.number]).corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, 
            annot=True,           # é¡¯ç¤ºæ•¸å€¼
            cmap='coolwarm',      # è‰²å½©æ˜ å°„
            center=0,             # ä»¥0ç‚ºä¸­å¿ƒ
            square=True,          # æ­£æ–¹å½¢æ ¼å­
            fmt='.2f')            # æ•¸å€¼æ ¼å¼
plt.title('è®Šæ•¸ç›¸é—œæ€§ç†±åŠ›åœ–')
plt.tight_layout()
plt.show()

# ğŸ“Š æˆå°é—œä¿‚åœ– - ä¸€æ¬¡çœ‹æ‰€æœ‰è®Šæ•¸çš„é—œä¿‚
g = sns.pairplot(tips, 
                 hue='time',           # æŒ‰æ™‚é–“åˆ†è‰²
                 diag_kind='kde',      # å°è§’ç·šç”¨å¯†åº¦åœ–
                 plot_kws={'alpha': 0.7})
g.fig.suptitle('æ‰€æœ‰è®Šæ•¸æˆå°é—œä¿‚åœ–', y=1.02)
plt.show()

# ğŸ’¡ Seaborn çš„å„ªå‹¢ï¼š
# 1. èªæ³•ç°¡æ½”ï¼šä¸€è¡Œç¨‹å¼ç¢¼å°±èƒ½ç¹ªè£½è¤‡é›œåœ–è¡¨
# 2. çµ±è¨ˆåŠŸèƒ½ï¼šè‡ªå‹•è¨ˆç®—çµ±è¨ˆé‡ï¼ˆå¦‚å›æ­¸ç·šã€ä¿¡è³´å€é–“ï¼‰
# 3. åˆ†çµ„åŠŸèƒ½ï¼šç”¨ hue, size, style åƒæ•¸è¼•é¬†åˆ†çµ„
# 4. ç¾è§€é è¨­ï¼šä¸éœ€èª¿æ•´å°±æœ‰å°ˆæ¥­å¤–è§€

# ğŸ¯ å¸¸ç”¨çš„ Seaborn åœ–è¡¨é¸æ“‡æŒ‡å—ï¼š
# - å–®è®Šæ•¸åˆ†å¸ƒï¼šhistplot, kdeplot, boxplot
# - é›™è®Šæ•¸é—œä¿‚ï¼šscatterplot, regplot
# - é¡åˆ¥æ¯”è¼ƒï¼šboxplot, violinplot, barplot
# - å¤šè®Šæ•¸æ¢ç´¢ï¼špairplot, heatmap
```

### (å››) Plotly äº’å‹•è¦–è¦ºåŒ–

**ä»€éº¼æ˜¯ Plotlyï¼Ÿ**
- ğŸ–±ï¸ **äº’å‹•å¼åœ–è¡¨**ï¼šæ”¯æ´ç¸®æ”¾ã€å¹³ç§»ã€æ‡¸åœé¡¯ç¤ºè³‡è¨Š
- ğŸŒ **ç¶²é æ•´åˆ**ï¼šå¯è¼¸å‡º HTMLï¼ŒåµŒå…¥ç¶²é æˆ–å ±å‘Š
- ğŸ“± **éŸ¿æ‡‰å¼è¨­è¨ˆ**ï¼šåœ¨ä¸åŒè£ç½®ä¸Šéƒ½èƒ½è‰¯å¥½é¡¯ç¤º
- ğŸ¨ **å°ˆæ¥­å¤–è§€**ï¼šé©åˆè£½ä½œå±•ç¤ºç”¨çš„åœ–è¡¨

```python
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np

# ğŸ“Š æº–å‚™ç¤ºä¾‹è³‡æ–™
np.random.seed(42)
df_sales = pd.DataFrame({
    'month': pd.date_range('2023-01', periods=12, freq='M'),
    'product_A': np.random.randint(100, 200, 12),
    'product_B': np.random.randint(80, 180, 12),
    'region': ['North', 'South'] * 6
})

# ğŸ“ˆ æ•£é»åœ– - æ¢ç´¢é—œä¿‚
fig_scatter = px.scatter(
    df_sales, 
    x='product_A', 
    y='product_B',
    color='region',                    # æŒ‰åœ°å€åˆ†è‰²
    size='product_A',                  # é»çš„å¤§å°è¡¨ç¤º product_A éŠ·é‡
    hover_data=['month'],              # æ‡¸åœæ™‚é¡¯ç¤ºæœˆä»½
    title='ç”¢å“A vs ç”¢å“B éŠ·é‡é—œä¿‚åœ–',
    labels={'product_A': 'ç”¢å“AéŠ·é‡', 'product_B': 'ç”¢å“BéŠ·é‡'}
)

# ğŸ’¡ äº’å‹•åŠŸèƒ½ï¼š
# - æ‡¸åœé¡¯ç¤ºè©³ç´°è³‡è¨Š
# - é»æ“Šåœ–ä¾‹éš±è—/é¡¯ç¤ºé¡åˆ¥
# - ç¸®æ”¾å’Œå¹³ç§»
fig_scatter.show()

# ğŸ“ˆ æ™‚é–“åºåˆ—ç·šåœ– - é¡¯ç¤ºè¶¨å‹¢
# é‡æ–°æ•´ç†è³‡æ–™ç‚ºé•·æ ¼å¼
df_long = df_sales.melt(
    id_vars=['month', 'region'], 
    value_vars=['product_A', 'product_B'],
    var_name='product', 
    value_name='sales'
)

fig_line = px.line(
    df_long, 
    x='month', 
    y='sales',
    color='product',                   # æŒ‰ç”¢å“åˆ†è‰²
    line_dash='region',                # æŒ‰åœ°å€åˆ†ç·šå‹
    title='2023å¹´ç”¢å“éŠ·é‡è¶¨å‹¢',
    labels={'sales': 'éŠ·é‡', 'month': 'æœˆä»½'}
)

# æ·»åŠ è¨»é‡‹
fig_line.add_annotation(
    x='2023-06',
    y=150,
    text="6æœˆä¿ƒéŠ·æ´»å‹•",
    showarrow=True,
    arrowhead=2
)

fig_line.show()

# ğŸ“Š é•·æ¢åœ– - æ¯”è¼ƒé¡åˆ¥
monthly_total = df_sales.groupby(df_sales['month'].dt.strftime('%Y-%m')).sum().reset_index()

fig_bar = px.bar(
    monthly_total, 
    x='month', 
    y=['product_A', 'product_B'],
    title='æ¯æœˆç”¢å“éŠ·é‡æ¯”è¼ƒ',
    labels={'value': 'éŠ·é‡', 'month': 'æœˆä»½', 'variable': 'ç”¢å“'},
    color_discrete_map={'product_A': '#FF6B6B', 'product_B': '#4ECDC4'}
)

fig_bar.update_layout(
    xaxis_title="æœˆä»½",
    yaxis_title="éŠ·é‡",
    legend_title="ç”¢å“é¡å‹"
)

fig_bar.show()

# ğŸ¥§ åœ“é¤…åœ– - é¡¯ç¤ºçµ„æˆæ¯”ä¾‹
total_sales = {
    'product_A': df_sales['product_A'].sum(),
    'product_B': df_sales['product_B'].sum()
}

fig_pie = px.pie(
    values=list(total_sales.values()), 
    names=list(total_sales.keys()),
    title='å…¨å¹´ç”¢å“éŠ·é‡å æ¯”',
    color_discrete_map={'product_A': '#FF6B6B', 'product_B': '#4ECDC4'}
)

# è‡ªå®šç¾©æ¨£å¼
fig_pie.update_traces(
    textposition='inside', 
    textinfo='percent+label',
    hovertemplate='<b>%{label}</b><br>éŠ·é‡: %{value}<br>å æ¯”: %{percent}<extra></extra>'
)

fig_pie.show()

# ğŸŒ 3D æ•£é»åœ– - å¤šç¶­åº¦è¦–è¦ºåŒ–
df_3d = pd.DataFrame({
    'x': np.random.randn(100),
    'y': np.random.randn(100), 
    'z': np.random.randn(100),
    'category': np.random.choice(['A', 'B', 'C'], 100),
    'size': np.random.randint(10, 30, 100)
})

fig_3d = px.scatter_3d(
    df_3d, 
    x='x', 
    y='y', 
    z='z',
    color='category',
    size='size',
    title='3D æ•£é»åœ–ç¤ºä¾‹'
)

# è¨­å®šç›¸æ©Ÿè§’åº¦
fig_3d.update_layout(
    scene=dict(
        xaxis_title='Xè»¸',
        yaxis_title='Yè»¸',
        zaxis_title='Zè»¸'
    )
)

fig_3d.show()

# ğŸ”¥ é€²éšåŠŸèƒ½ï¼šå­åœ–å’Œå„€è¡¨æ¿
from plotly.subplots import make_subplots

# å»ºç«‹å­åœ–
fig_dashboard = make_subplots(
    rows=2, cols=2,
    subplot_titles=('æ™‚é–“è¶¨å‹¢', 'ç›¸é—œæ€§', 'åˆ†å¸ƒ', 'æ¯”è¼ƒ'),
    specs=[[{"secondary_y": True}, {"type": "scatter"}],
           [{"type": "histogram"}, {"type": "bar"}]]
)

# æ·»åŠ ä¸åŒé¡å‹çš„åœ–è¡¨
fig_dashboard.add_trace(
    go.Scatter(x=df_sales['month'], y=df_sales['product_A'], name='ç”¢å“A'),
    row=1, col=1
)

fig_dashboard.add_trace(
    go.Scatter(x=df_sales['product_A'], y=df_sales['product_B'], 
               mode='markers', name='A vs B'),
    row=1, col=2
)

fig_dashboard.add_trace(
    go.Histogram(x=df_sales['product_A'], name='ç”¢å“Aåˆ†å¸ƒ'),
    row=2, col=1
)

fig_dashboard.add_trace(
    go.Bar(x=['ç”¢å“A', 'ç”¢å“B'], 
           y=[df_sales['product_A'].mean(), df_sales['product_B'].mean()],
           name='å¹³å‡éŠ·é‡'),
    row=2, col=2
)

fig_dashboard.update_layout(
    title_text="éŠ·é‡åˆ†æå„€è¡¨æ¿",
    showlegend=False,
    height=600
)

fig_dashboard.show()

# ğŸ’¡ Plotly çš„å„ªå‹¢ï¼š
# 1. äº’å‹•æ€§ï¼šä½¿ç”¨è€…å¯ä»¥æ¢ç´¢è³‡æ–™
# 2. å°ˆæ¥­å¤–è§€ï¼šé©åˆå•†æ¥­å ±å‘Š
# 3. æ˜“æ–¼åˆ†äº«ï¼šè¼¸å‡º HTML æ–‡ä»¶
# 4. éŸ¿æ‡‰å¼ï¼šé©æ‡‰ä¸åŒè¢å¹•å¤§å°

# ğŸ¯ ä½¿ç”¨å»ºè­°ï¼š
# - æ¢ç´¢æ€§åˆ†æï¼šç”¨ px (plotly express) å¿«é€Ÿè£½åœ–
# - å®¢è£½åŒ–éœ€æ±‚ï¼šç”¨ go (graph objects) ç²¾ç´°æ§åˆ¶
# - å ±å‘Šå±•ç¤ºï¼šåŠ å…¥è¨»é‡‹å’Œè‡ªå®šç¾©æ¨£å¼
# - ç¶²é æ•´åˆï¼šè¼¸å‡º HTML åµŒå…¥ç¶²ç«™

# ğŸ’¾ å„²å­˜åœ–è¡¨
# fig.write_html("my_plot.html")        # å„²å­˜ç‚º HTML
# fig.write_image("my_plot.png")        # å„²å­˜ç‚ºåœ–ç‰‡ï¼ˆéœ€å®‰è£ kaleidoï¼‰
```

## ä¸‰ã€çµ±è¨ˆåˆ†æ

### (ä¸€) çµ±è¨ˆåˆ†æçš„é‡è¦æ€§

**ç‚ºä»€éº¼éœ€è¦çµ±è¨ˆåˆ†æï¼Ÿ**
- ğŸ” **ç™¼ç¾æ¨¡å¼**ï¼šå¾çœ‹ä¼¼éš¨æ©Ÿçš„è³‡æ–™ä¸­æ‰¾å‡ºè¦å¾‹
- ğŸ“Š **é‡åŒ–é—œä¿‚**ï¼šæ¸¬é‡è®Šæ•¸é–“çš„é—œè¯å¼·åº¦
- ğŸ¯ **é©—è­‰å‡è¨­**ï¼šç”¨æ•¸æ“šè­‰æ˜æˆ–æ¨ç¿»æˆ‘å€‘çš„æƒ³æ³•
- ğŸ”® **é æ¸¬æœªä¾†**ï¼šåŸºæ–¼æ­·å²è³‡æ–™é æ¸¬è¶¨å‹¢

**çµ±è¨ˆåˆ†æçš„å±¤æ¬¡**

| å±¤æ¬¡ | ç›®çš„ | ç¯„ä¾‹å•é¡Œ | ä½¿ç”¨æ–¹æ³• |
|------|------|----------|----------|
| æè¿°çµ±è¨ˆ | ç¸½çµè³‡æ–™ç‰¹å¾µ | å¹³å‡å¹´è–ªæ˜¯å¤šå°‘ï¼Ÿ | å¹³å‡å€¼ã€ä¸­ä½æ•¸ |
| æ¨è«–çµ±è¨ˆ | å¾æ¨£æœ¬æ¨è«–æ¯é«” | æ–°è—¥æ˜¯å¦æœ‰æ•ˆï¼Ÿ | å‡è¨­æª¢å®š |
| é æ¸¬åˆ†æ | é æ¸¬æœªä¾†è¶¨å‹¢ | æ˜å¹´éŠ·é‡æœƒæ˜¯å¤šå°‘ï¼Ÿ | è¿´æ­¸åˆ†æ |

### (äºŒ) æè¿°çµ±è¨ˆ

**æè¿°çµ±è¨ˆçš„ç›®æ¨™ï¼šç”¨å¹¾å€‹æ•¸å­—ç¸½çµæ•´å€‹è³‡æ–™é›†**

```python
import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“Š æº–å‚™ç¤ºä¾‹è³‡æ–™ - å“¡å·¥è–ªè³‡è³‡æ–™
np.random.seed(42)
salaries = np.random.normal(60000, 15000, 1000)  # å¹³å‡60000ï¼Œæ¨™æº–å·®15000
salaries = np.clip(salaries, 30000, 120000)      # é™åˆ¶åœ¨åˆç†ç¯„åœå…§

# 1ï¸âƒ£ ä¸­å¿ƒè¶¨å‹¢ - è³‡æ–™çš„ã€Œä¸­å¿ƒã€åœ¨å“ªè£¡ï¼Ÿ
mean_salary = np.mean(salaries)         # å¹³å‡å€¼ï¼šæ‰€æœ‰æ•¸çš„ç¸½å’Œé™¤ä»¥å€‹æ•¸
median_salary = np.median(salaries)     # ä¸­ä½æ•¸ï¼šæ’åºå¾Œä¸­é–“çš„æ•¸
mode_result = stats.mode(salaries)      # çœ¾æ•¸ï¼šæœ€å¸¸å‡ºç¾çš„æ•¸

print("=== ä¸­å¿ƒè¶¨å‹¢åˆ†æ ===")
print(f"å¹³å‡è–ªè³‡ï¼š${mean_salary:,.0f}")
print(f"ä¸­ä½æ•¸è–ªè³‡ï¼š${median_salary:,.0f}")
print(f"è–ªè³‡çœ¾æ•¸ï¼š${mode_result.mode[0]:,.0f}")

# ğŸ’¡ ä»€éº¼æ™‚å€™ç”¨å“ªå€‹ï¼Ÿ
# - å¹³å‡å€¼ï¼šè³‡æ–™åˆ†å¸ƒå°ç¨±æ™‚ä½¿ç”¨
# - ä¸­ä½æ•¸ï¼šæœ‰æ¥µç«¯å€¼æ™‚æ›´ç©©å¥
# - çœ¾æ•¸ï¼šé¡åˆ¥è³‡æ–™æˆ–æƒ³çŸ¥é“æœ€å¸¸è¦‹å€¼

# è¦–è¦ºåŒ–ä¸­å¿ƒè¶¨å‹¢
plt.figure(figsize=(10, 6))
plt.hist(salaries, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(mean_salary, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡å€¼: ${mean_salary:,.0f}')
plt.axvline(median_salary, color='green', linestyle='--', linewidth=2, label=f'ä¸­ä½æ•¸: ${median_salary:,.0f}')
plt.xlabel('è–ªè³‡ ($)')
plt.ylabel('å“¡å·¥æ•¸')
plt.title('å“¡å·¥è–ªè³‡åˆ†å¸ƒèˆ‡ä¸­å¿ƒè¶¨å‹¢')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 2ï¸âƒ£ é›¢æ•£ç¨‹åº¦ - è³‡æ–™æ•£å¸ƒå¾—å¤šå»£ï¼Ÿ
variance = np.var(salaries)             # è®Šç•°æ•¸ï¼šå¹³å‡åå·®çš„å¹³æ–¹
std_dev = np.std(salaries)              # æ¨™æº–å·®ï¼šè®Šç•°æ•¸çš„å¹³æ–¹æ ¹
cv = std_dev / mean_salary              # è®Šç•°ä¿‚æ•¸ï¼šæ¨™æº–åŒ–çš„é›¢æ•£ç¨‹åº¦
range_val = np.max(salaries) - np.min(salaries)  # å…¨è·ï¼šæœ€å¤§å€¼æ¸›æœ€å°å€¼

print("\n=== é›¢æ•£ç¨‹åº¦åˆ†æ ===")
print(f"æ¨™æº–å·®ï¼š${std_dev:,.0f}")
print(f"è®Šç•°æ•¸ï¼š${variance:,.0f}")
print(f"è®Šç•°ä¿‚æ•¸ï¼š{cv:.2%}")
print(f"å…¨è·ï¼š${range_val:,.0f}")

# ğŸ’¡ æ¨™æº–å·®çš„æ„ç¾©ï¼š
# - ç´„68%çš„è³‡æ–™åœ¨ å¹³å‡å€¼ Â± 1å€‹æ¨™æº–å·® å…§
# - ç´„95%çš„è³‡æ–™åœ¨ å¹³å‡å€¼ Â± 2å€‹æ¨™æº–å·® å…§
lower_1std = mean_salary - std_dev
upper_1std = mean_salary + std_dev
within_1std = np.sum((salaries >= lower_1std) & (salaries <= upper_1std)) / len(salaries)
print(f"åœ¨1å€‹æ¨™æº–å·®å…§çš„è³‡æ–™æ¯”ä¾‹ï¼š{within_1std:.1%}")

# 3ï¸âƒ£ åˆ†å¸ƒå½¢ç‹€ç‰¹å¾µ
skewness = stats.skew(salaries)         # ååº¦ï¼šåˆ†å¸ƒæ˜¯å¦å°ç¨±
kurtosis = stats.kurtosis(salaries)     # å³°åº¦ï¼šåˆ†å¸ƒçš„å°–éŠ³ç¨‹åº¦

print("\n=== åˆ†å¸ƒå½¢ç‹€ç‰¹å¾µ ===")
print(f"ååº¦ï¼š{skewness:.3f}")
print(f"å³°åº¦ï¼š{kurtosis:.3f}")

# ğŸ’¡ ååº¦è§£è®€ï¼š
# - ååº¦ = 0ï¼šå°ç¨±åˆ†å¸ƒ
# - ååº¦ > 0ï¼šå³åï¼ˆé•·å°¾åœ¨å³é‚Šï¼‰
# - ååº¦ < 0ï¼šå·¦åï¼ˆé•·å°¾åœ¨å·¦é‚Šï¼‰

if skewness > 0.5:
    skew_desc = "å³åï¼ˆé«˜è–ªå“¡å·¥è¼ƒå°‘ï¼‰"
elif skewness < -0.5:
    skew_desc = "å·¦åï¼ˆä½è–ªå“¡å·¥è¼ƒå°‘ï¼‰"
else:
    skew_desc = "è¿‘ä¼¼å°ç¨±åˆ†å¸ƒ"

print(f"åˆ†å¸ƒç‰¹å¾µï¼š{skew_desc}")

# 4ï¸âƒ£ ç™¾åˆ†ä½æ•¸ - è³‡æ–™çš„åˆ†ä½é»
percentiles = np.percentile(salaries, [10, 25, 50, 75, 90])
q1, q3 = np.percentile(salaries, [25, 75])
iqr = q3 - q1  # å››åˆ†ä½è·

print("\n=== ç™¾åˆ†ä½æ•¸åˆ†æ ===")
print(f"ç¬¬10ç™¾åˆ†ä½ï¼š${percentiles[0]:,.0f}")
print(f"ç¬¬25ç™¾åˆ†ä½(Q1)ï¼š${percentiles[1]:,.0f}")
print(f"ç¬¬50ç™¾åˆ†ä½(ä¸­ä½æ•¸)ï¼š${percentiles[2]:,.0f}")
print(f"ç¬¬75ç™¾åˆ†ä½(Q3)ï¼š${percentiles[3]:,.0f}")
print(f"ç¬¬90ç™¾åˆ†ä½ï¼š${percentiles[4]:,.0f}")
print(f"å››åˆ†ä½è·(IQR)ï¼š${iqr:,.0f}")

# ğŸ’¡ ç™¾åˆ†ä½æ•¸çš„å¯¦éš›æ„ç¾©ï¼š
# - ç¬¬25ç™¾åˆ†ä½ï¼š25%çš„å“¡å·¥è–ªè³‡ä½æ–¼æ­¤å€¼
# - ç¬¬75ç™¾åˆ†ä½ï¼š75%çš„å“¡å·¥è–ªè³‡ä½æ–¼æ­¤å€¼
# - IQRï¼šä¸­é–“50%å“¡å·¥çš„è–ªè³‡ç¯„åœ

# 5ï¸âƒ£ ä¸€æ¬¡æ€§å®Œæ•´çµ±è¨ˆæ‘˜è¦
df_salaries = pd.DataFrame({'salary': salaries})
summary_stats = df_salaries.describe()

print("\n=== å®Œæ•´çµ±è¨ˆæ‘˜è¦ ===")
print(summary_stats)

# ç®±å½¢åœ–è¦–è¦ºåŒ–
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.boxplot(salaries)
plt.ylabel('è–ªè³‡ ($)')
plt.title('è–ªè³‡ç®±å½¢åœ–')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(salaries, bins=50, alpha=0.7, color='lightcoral')
plt.xlabel('è–ªè³‡ ($)')
plt.ylabel('é »ç‡')
plt.title('è–ªè³‡åˆ†å¸ƒç›´æ–¹åœ–')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### (ä¸‰) å‡è¨­æª¢å®š

**ä»€éº¼æ˜¯å‡è¨­æª¢å®šï¼Ÿ**
- ğŸ¯ **ç§‘å­¸æ–¹æ³•**ï¼šç”¨è³‡æ–™è­‰æ˜æˆ–æ¨ç¿»æˆ‘å€‘çš„çŒœæ¸¬
- ğŸ“Š **æ±ºç­–å·¥å…·**ï¼šå¹«åŠ©åšå‡ºåŸºæ–¼è­‰æ“šçš„æ±ºå®š
- ğŸ” **é¢¨éšªæ§åˆ¶**ï¼šé‡åŒ–éŒ¯èª¤æ±ºç­–çš„æ©Ÿç‡

**å‡è¨­æª¢å®šçš„åŸºæœ¬æ¦‚å¿µ**

| æ¦‚å¿µ | èªªæ˜ | ç¯„ä¾‹ |
|------|------|------|
| è™›ç„¡å‡è¨­(Hâ‚€) | æˆ‘å€‘æƒ³è¦æ¨ç¿»çš„å‡è¨­ | æ–°è—¥æ²’æœ‰æ•ˆæœ |
| å°ç«‹å‡è¨­(Hâ‚) | æˆ‘å€‘æƒ³è¦è­‰æ˜çš„å‡è¨­ | æ–°è—¥æœ‰æ•ˆæœ |
| på€¼ | åœ¨Hâ‚€ç‚ºçœŸä¸‹ï¼Œè§€å¯Ÿåˆ°æ­¤çµæœçš„æ©Ÿç‡ | p = 0.03 |
| é¡¯è‘—æ°´æº–(Î±) | æˆ‘å€‘èƒ½æ¥å—çš„éŒ¯èª¤æ©Ÿç‡ | Î± = 0.05 |

```python
import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ğŸ§ª æº–å‚™å¯¦é©—è³‡æ–™
np.random.seed(42)

# 1ï¸âƒ£ å¸¸æ…‹æ€§æª¢å®š - è³‡æ–™æ˜¯å¦ç¬¦åˆå¸¸æ…‹åˆ†å¸ƒï¼Ÿ
# ç‚ºä»€éº¼é‡è¦ï¼Ÿå¾ˆå¤šçµ±è¨ˆæ–¹æ³•å‡è¨­è³‡æ–™æ˜¯å¸¸æ…‹åˆ†å¸ƒçš„

# ç”Ÿæˆä¸åŒåˆ†å¸ƒçš„è³‡æ–™
normal_data = np.random.normal(50, 10, 1000)      # å¸¸æ…‹åˆ†å¸ƒ
uniform_data = np.random.uniform(30, 70, 1000)    # å‡å‹»åˆ†å¸ƒ
skewed_data = np.random.exponential(2, 1000)      # æŒ‡æ•¸åˆ†å¸ƒï¼ˆå³åï¼‰

# Shapiro-Wilk æª¢å®šï¼ˆæ¨£æœ¬æ•¸ < 5000 æ™‚ä½¿ç”¨ï¼‰
def test_normality(data, name):
    statistic, p_value = stats.shapiro(data)
    print(f"\n=== {name} å¸¸æ…‹æ€§æª¢å®š ===")
    print(f"Shapiro-Wilk çµ±è¨ˆé‡ï¼š{statistic:.4f}")
    print(f"på€¼ï¼š{p_value:.6f}")
    
    if p_value > 0.05:
        print("âœ… çµè«–ï¼šè³‡æ–™ç¬¦åˆå¸¸æ…‹åˆ†å¸ƒï¼ˆp > 0.05ï¼‰")
    else:
        print("âŒ çµè«–ï¼šè³‡æ–™ä¸ç¬¦åˆå¸¸æ…‹åˆ†å¸ƒï¼ˆp â‰¤ 0.05ï¼‰")
    
    return p_value

# æª¢å®šä¸åŒè³‡æ–™
p_normal = test_normality(normal_data, "å¸¸æ…‹åˆ†å¸ƒè³‡æ–™")
p_uniform = test_normality(uniform_data, "å‡å‹»åˆ†å¸ƒè³‡æ–™")  
p_skewed = test_normality(skewed_data, "å³ååˆ†å¸ƒè³‡æ–™")

# è¦–è¦ºåŒ–æ¯”è¼ƒ
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].hist(normal_data, bins=50, alpha=0.7, color='blue')
axes[0].set_title(f'å¸¸æ…‹åˆ†å¸ƒ (p={p_normal:.4f})')
axes[0].set_ylabel('é »ç‡')

axes[1].hist(uniform_data, bins=50, alpha=0.7, color='green')
axes[1].set_title(f'å‡å‹»åˆ†å¸ƒ (p={p_uniform:.4f})')

axes[2].hist(skewed_data, bins=50, alpha=0.7, color='red')
axes[2].set_title(f'å³ååˆ†å¸ƒ (p={p_skewed:.4f})')

plt.tight_layout()
plt.show()

# 2ï¸âƒ£ tæª¢å®š - æ¯”è¼ƒå¹³å‡å€¼çš„å·®ç•°
print("\n" + "="*50)
print("Tæª¢å®šç¯„ä¾‹")
print("="*50)

# æƒ…å¢ƒï¼šæ¯”è¼ƒå…©çµ„å­¸ç”Ÿçš„è€ƒè©¦æˆç¸¾
group_A = np.random.normal(75, 8, 30)  # Açµ„ï¼šå¹³å‡75åˆ†
group_B = np.random.normal(80, 8, 30)  # Bçµ„ï¼šå¹³å‡80åˆ†

# ç¨ç«‹æ¨£æœ¬ tæª¢å®š
t_stat, p_value = stats.ttest_ind(group_A, group_B)

print(f"Açµ„å¹³å‡åˆ†æ•¸ï¼š{np.mean(group_A):.2f}")
print(f"Bçµ„å¹³å‡åˆ†æ•¸ï¼š{np.mean(group_B):.2f}")
print(f"å¹³å‡åˆ†æ•¸å·®ç•°ï¼š{np.mean(group_B) - np.mean(group_A):.2f}")
print(f"\nç¨ç«‹æ¨£æœ¬tæª¢å®šçµæœï¼š")
print(f"tçµ±è¨ˆé‡ï¼š{t_stat:.4f}")
print(f"på€¼ï¼š{p_value:.6f}")

if p_value < 0.05:
    print("âœ… çµè«–ï¼šå…©çµ„æˆç¸¾æœ‰é¡¯è‘—å·®ç•°ï¼ˆp < 0.05ï¼‰")
else:
    print("âŒ çµè«–ï¼šå…©çµ„æˆç¸¾æ²’æœ‰é¡¯è‘—å·®ç•°ï¼ˆp â‰¥ 0.05ï¼‰")

# é…å°æ¨£æœ¬ tæª¢å®šï¼ˆåŒä¸€ç¾¤äººçš„å‰å¾Œæ¸¬ï¼‰
before_training = np.random.normal(70, 10, 25)
after_training = before_training + np.random.normal(5, 3, 25)  # è¨“ç·´å¾Œæå‡

paired_t_stat, paired_p_value = stats.ttest_rel(before_training, after_training)

print(f"\n=== é…å°æ¨£æœ¬tæª¢å®šï¼ˆè¨“ç·´å‰å¾Œæ¯”è¼ƒï¼‰===")
print(f"è¨“ç·´å‰å¹³å‡ï¼š{np.mean(before_training):.2f}")
print(f"è¨“ç·´å¾Œå¹³å‡ï¼š{np.mean(after_training):.2f}")
print(f"tçµ±è¨ˆé‡ï¼š{paired_t_stat:.4f}")
print(f"på€¼ï¼š{paired_p_value:.6f}")

if paired_p_value < 0.05:
    print("âœ… çµè«–ï¼šè¨“ç·´æœ‰é¡¯è‘—æ•ˆæœï¼ˆp < 0.05ï¼‰")
else:
    print("âŒ çµè«–ï¼šè¨“ç·´æ²’æœ‰é¡¯è‘—æ•ˆæœï¼ˆp â‰¥ 0.05ï¼‰")

# 3ï¸âƒ£ è®Šç•°æ•¸åˆ†æï¼ˆANOVAï¼‰- æ¯”è¼ƒå¤šå€‹ç¾¤çµ„
print(f"\n=== å–®å› å­è®Šç•°æ•¸åˆ†æï¼ˆæ¯”è¼ƒå¤šå€‹ç¾¤çµ„ï¼‰===")

# æƒ…å¢ƒï¼šæ¯”è¼ƒä¸‰ç¨®æ•™å­¸æ–¹æ³•çš„æ•ˆæœ
method_A = np.random.normal(75, 8, 20)  # å‚³çµ±æ•™å­¸
method_B = np.random.normal(80, 8, 20)  # ç·šä¸Šæ•™å­¸
method_C = np.random.normal(82, 8, 20)  # æ··åˆæ•™å­¸

f_stat, anova_p_value = stats.f_oneway(method_A, method_B, method_C)

print(f"å‚³çµ±æ•™å­¸å¹³å‡ï¼š{np.mean(method_A):.2f}")
print(f"ç·šä¸Šæ•™å­¸å¹³å‡ï¼š{np.mean(method_B):.2f}")
print(f"æ··åˆæ•™å­¸å¹³å‡ï¼š{np.mean(method_C):.2f}")
print(f"\nANOVAæª¢å®šçµæœï¼š")
print(f"Fçµ±è¨ˆé‡ï¼š{f_stat:.4f}")
print(f"på€¼ï¼š{anova_p_value:.6f}")

if anova_p_value < 0.05:
    print("âœ… çµè«–ï¼šè‡³å°‘æœ‰ä¸€ç¨®æ•™å­¸æ–¹æ³•æ•ˆæœé¡¯è‘—ä¸åŒï¼ˆp < 0.05ï¼‰")
else:
    print("âŒ çµè«–ï¼šä¸‰ç¨®æ•™å­¸æ–¹æ³•æ•ˆæœæ²’æœ‰é¡¯è‘—å·®ç•°ï¼ˆp â‰¥ 0.05ï¼‰")

# 4ï¸âƒ£ å¡æ–¹æª¢å®š - åˆ†æé¡åˆ¥è®Šæ•¸çš„é—œè¯
print(f"\n=== å¡æ–¹æª¢å®šï¼ˆé¡åˆ¥è®Šæ•¸ç¨ç«‹æ€§ï¼‰===")

# æƒ…å¢ƒï¼šæ€§åˆ¥èˆ‡ç”¢å“åå¥½æ˜¯å¦æœ‰é—œè¯ï¼Ÿ
# å»ºç«‹åˆ—è¯è¡¨
observed = np.array([
    [20, 30, 10],  # ç”·æ€§å°ç”¢å“Aã€Bã€Cçš„åå¥½
    [15, 25, 20]   # å¥³æ€§å°ç”¢å“Aã€Bã€Cçš„åå¥½
])

chi2_stat, chi2_p_value, dof, expected = stats.chi2_contingency(observed)

print("è§€å¯Ÿé »ç‡è¡¨ï¼ˆæ€§åˆ¥ vs ç”¢å“åå¥½ï¼‰ï¼š")
print("       ç”¢å“A  ç”¢å“B  ç”¢å“C")
print(f"ç”·æ€§     {observed[0,0]}     {observed[0,1]}     {observed[0,2]}")
print(f"å¥³æ€§     {observed[1,0]}     {observed[1,1]}     {observed[1,2]}")

print(f"\nå¡æ–¹æª¢å®šçµæœï¼š")
print(f"å¡æ–¹çµ±è¨ˆé‡ï¼š{chi2_stat:.4f}")
print(f"è‡ªç”±åº¦ï¼š{dof}")
print(f"på€¼ï¼š{chi2_p_value:.6f}")

if chi2_p_value < 0.05:
    print("âœ… çµè«–ï¼šæ€§åˆ¥èˆ‡ç”¢å“åå¥½æœ‰é¡¯è‘—é—œè¯ï¼ˆp < 0.05ï¼‰")
else:
    print("âŒ çµè«–ï¼šæ€§åˆ¥èˆ‡ç”¢å“åå¥½æ²’æœ‰é¡¯è‘—é—œè¯ï¼ˆp â‰¥ 0.05ï¼‰")

# 5ï¸âƒ£ ç›¸é—œæ€§åˆ†æ - æ¸¬é‡è®Šæ•¸é–“çš„ç·šæ€§é—œä¿‚
print(f"\n=== ç›¸é—œæ€§åˆ†æ ===")

# ç”Ÿæˆæœ‰ç›¸é—œæ€§çš„è³‡æ–™
x = np.random.normal(0, 1, 100)
y_strong = 0.8 * x + np.random.normal(0, 0.5, 100)    # å¼·æ­£ç›¸é—œ
y_weak = 0.3 * x + np.random.normal(0, 1, 100)       # å¼±æ­£ç›¸é—œ

# Pearson ç›¸é—œä¿‚æ•¸ï¼ˆç·šæ€§ç›¸é—œï¼‰
pearson_strong, p_pearson_strong = stats.pearsonr(x, y_strong)
pearson_weak, p_pearson_weak = stats.pearsonr(x, y_weak)

# Spearman ç›¸é—œä¿‚æ•¸ï¼ˆå–®èª¿ç›¸é—œï¼Œå°ç•°å¸¸å€¼è¼ƒç©©å¥ï¼‰
spearman_strong, p_spearman_strong = stats.spearmanr(x, y_strong)
spearman_weak, p_spearman_weak = stats.spearmanr(x, y_weak)

print("å¼·ç›¸é—œè³‡æ–™ï¼š")
print(f"  Pearsonç›¸é—œä¿‚æ•¸ï¼š{pearson_strong:.4f} (p={p_pearson_strong:.6f})")
print(f"  Spearmanç›¸é—œä¿‚æ•¸ï¼š{spearman_strong:.4f} (p={p_spearman_strong:.6f})")

print("å¼±ç›¸é—œè³‡æ–™ï¼š")
print(f"  Pearsonç›¸é—œä¿‚æ•¸ï¼š{pearson_weak:.4f} (p={p_pearson_weak:.6f})")
print(f"  Spearmanç›¸é—œä¿‚æ•¸ï¼š{spearman_weak:.4f} (p={p_spearman_weak:.6f})")

# ğŸ’¡ ç›¸é—œä¿‚æ•¸è§£è®€æŒ‡å—ï¼š
print(f"\nğŸ’¡ ç›¸é—œä¿‚æ•¸è§£è®€ï¼š")
print("  |r| < 0.3   ï¼šå¼±ç›¸é—œ")
print("  0.3 â‰¤ |r| < 0.7ï¼šä¸­ç­‰ç›¸é—œ") 
print("  |r| â‰¥ 0.7   ï¼šå¼·ç›¸é—œ")

# è¦–è¦ºåŒ–ç›¸é—œæ€§
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.scatter(x, y_strong, alpha=0.6)
ax1.set_title(f'å¼·ç›¸é—œ (r={pearson_strong:.3f})')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.grid(True, alpha=0.3)

ax2.scatter(x, y_weak, alpha=0.6)
ax2.set_title(f'å¼±ç›¸é—œ (r={pearson_weak:.3f})')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ğŸ’¡ å‡è¨­æª¢å®šå°çµï¼š
print(f"\nğŸ’¡ å‡è¨­æª¢å®šä½¿ç”¨æŒ‡å—ï¼š")
print("ğŸ“Š å¸¸æ…‹æ€§æª¢å®šï¼šæ±ºå®šä½¿ç”¨åƒæ•¸æˆ–éåƒæ•¸æª¢å®š")
print("ğŸ“Š tæª¢å®šï¼šæ¯”è¼ƒ1-2å€‹ç¾¤çµ„çš„å¹³å‡å€¼")
print("ğŸ“Š ANOVAï¼šæ¯”è¼ƒ3å€‹ä»¥ä¸Šç¾¤çµ„çš„å¹³å‡å€¼")
print("ğŸ“Š å¡æ–¹æª¢å®šï¼šåˆ†æé¡åˆ¥è®Šæ•¸çš„é—œè¯æ€§")
print("ğŸ“Š ç›¸é—œåˆ†æï¼šæ¸¬é‡é€£çºŒè®Šæ•¸çš„ç·šæ€§é—œä¿‚")
```

### è¿´æ­¸åˆ†æ
```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# ç·šæ€§è¿´æ­¸
model = LinearRegression()
model.fit(X, y)

# é æ¸¬
y_pred = model.predict(X_test)

# æ¨¡å‹è©•ä¼°
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# æ¨¡å‹ä¿‚æ•¸
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
})

### (å››) è¿´æ­¸åˆ†æé€²éšè©³è§£

**ä»€éº¼æ˜¯è¿´æ­¸åˆ†æï¼Ÿ**
- ğŸ¯ **é æ¸¬å·¥å…·**ï¼šæ ¹æ“šå·²çŸ¥è®Šæ•¸é æ¸¬æœªçŸ¥çµæœ
- ğŸ“ˆ **é—œä¿‚å»ºæ¨¡**ï¼šé‡åŒ–è®Šæ•¸é–“çš„å½±éŸ¿é—œä¿‚
- ğŸ’° **å•†æ¥­åƒ¹å€¼**ï¼šéŠ·é‡é æ¸¬ã€é¢¨éšªè©•ä¼°ã€å®šåƒ¹ç­–ç•¥
- ğŸ”¬ **ç§‘å­¸ç ”ç©¶**ï¼šé©—è­‰ç†è«–ã€æ¢ç´¢å› æœé—œä¿‚

**è¿´æ­¸åˆ†æçš„é¡å‹èˆ‡æ‡‰ç”¨**

| é¡å‹ | é©ç”¨æƒ…æ³ | å•†æ¥­æ‡‰ç”¨ç¯„ä¾‹ | æŠ€è¡“ç‰¹é» |
|------|----------|--------------|----------|
| ç°¡å–®ç·šæ€§è¿´æ­¸ | ä¸€å°ä¸€è®Šæ•¸é—œä¿‚ | å»£å‘Šæ”¯å‡º â†’ éŠ·é‡ | æ˜“æ–¼è§£é‡‹ |
| å¤šå…ƒç·šæ€§è¿´æ­¸ | å¤šå› ç´ å½±éŸ¿ | æˆ¿å±‹ç‰¹å¾µ â†’ åƒ¹æ ¼ | æ§åˆ¶å…¶ä»–è®Šæ•¸ |
| é‚è¼¯è¿´æ­¸ | äºŒå…ƒåˆ†é¡ | å®¢æˆ¶æµå¤±é æ¸¬ | æ©Ÿç‡è¼¸å‡º |
| å¤šé …å¼è¿´æ­¸ | éç·šæ€§é—œä¿‚ | æº«åº¦èˆ‡é›»åŠ›éœ€æ±‚ | è™•ç†æ›²ç·šé—œä¿‚ |

```python
# ğŸ“Š å®Œæ•´è¿´æ­¸åˆ†æå¯¦ä¾‹ï¼šæˆ¿åƒ¹é æ¸¬

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# 1ï¸âƒ£ å»ºç«‹æ¨¡æ“¬æˆ¿å±‹è³‡æ–™
np.random.seed(42)
n_houses = 500

house_data = pd.DataFrame({
    'size': np.random.normal(1500, 400, n_houses),           # æˆ¿å±‹é¢ç©ï¼ˆå¹³æ–¹è‹±å°ºï¼‰
    'bedrooms': np.random.randint(1, 6, n_houses),           # è‡¥å®¤æ•¸é‡
    'bathrooms': np.random.randint(1, 4, n_houses),          # æµ´å®¤æ•¸é‡
    'age': np.random.randint(0, 30, n_houses),               # æˆ¿é½¡ï¼ˆå¹´ï¼‰
    'location_score': np.random.uniform(1, 10, n_houses),    # ä½ç½®è©•åˆ†ï¼ˆ1-10ï¼‰
    'garage': np.random.choice([0, 1, 2], n_houses)          # è»Šåº«æ•¸é‡
})

# ç¢ºä¿è³‡æ–™åœ¨åˆç†ç¯„åœå…§
house_data['size'] = np.clip(house_data['size'], 800, 3000)

# 2ï¸âƒ£ ç”Ÿæˆæˆ¿åƒ¹ï¼ˆåŸºæ–¼å¤šå€‹å› ç´ çš„çœŸå¯¦é—œä¿‚ï¼‰
house_data['price'] = (
    150 * house_data['size'] +                    # é¢ç©ï¼šæ¯å¹³æ–¹è‹±å°º$150
    8000 * house_data['bedrooms'] +               # è‡¥å®¤ï¼šæ¯é–“$8000
    5000 * house_data['bathrooms'] +              # æµ´å®¤ï¼šæ¯é–“$5000
    -800 * house_data['age'] +                    # æˆ¿é½¡ï¼šæ¯å¹´è²¶å€¼$800
    3000 * house_data['location_score'] +         # ä½ç½®ï¼šæ¯åˆ†$3000
    10000 * house_data['garage'] +                # è»Šåº«ï¼šæ¯å€‹$10000
    50000 +                                       # åŸºç¤åƒ¹æ ¼
    np.random.normal(0, 15000, n_houses)          # éš¨æ©Ÿèª¤å·®
)

# ç¢ºä¿æˆ¿åƒ¹åœ¨åˆç†ç¯„åœ
house_data['price'] = np.clip(house_data['price'], 100000, 800000)

print("ğŸ“Š æˆ¿å±‹è³‡æ–™æ¦‚è¦½ï¼š")
print(house_data.describe())

# 3ï¸âƒ£ è³‡æ–™æº–å‚™å’Œåˆ†å‰²
features = ['size', 'bedrooms', 'bathrooms', 'age', 'location_score', 'garage']
X = house_data[features]
y = house_data['price']

# åˆ†å‰²è³‡æ–™ï¼ˆ70%è¨“ç·´ï¼Œ30%æ¸¬è©¦ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"\nğŸ“ˆ è³‡æ–™åˆ†å‰²çµæœï¼š")
print(f"è¨“ç·´é›†ï¼š{len(X_train)} ç­†è³‡æ–™")
print(f"æ¸¬è©¦é›†ï¼š{len(X_test)} ç­†è³‡æ–™")

# 4ï¸âƒ£ å»ºç«‹å’Œè¨“ç·´æ¨¡å‹
model = LinearRegression()
model.fit(X_train, y_train)

# 5ï¸âƒ£ æ¨¡å‹é æ¸¬å’Œè©•ä¼°
y_pred_train = model.predict(X_train)  # è¨“ç·´é›†é æ¸¬
y_pred_test = model.predict(X_test)    # æ¸¬è©¦é›†é æ¸¬

# è¨ˆç®—è©•ä¼°æŒ‡æ¨™
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
test_mae = mean_absolute_error(y_test, y_pred_test)

print(f"\nğŸ“Š æ¨¡å‹æ•ˆèƒ½è©•ä¼°ï¼š")
print(f"è¨“ç·´é›† RÂ²ï¼š{train_r2:.4f}")
print(f"æ¸¬è©¦é›† RÂ²ï¼š{test_r2:.4f}")
print(f"æ¸¬è©¦é›† RMSEï¼š${test_rmse:,.0f}")
print(f"æ¸¬è©¦é›† MAEï¼š${test_mae:,.0f}")

# æª¢æŸ¥éåº¦é…é©
if train_r2 - test_r2 > 0.1:
    print("âš ï¸ è­¦å‘Šï¼šå¯èƒ½å­˜åœ¨éåº¦é…é©")
else:
    print("âœ… æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")

# 6ï¸âƒ£ ç‰¹å¾µé‡è¦æ€§åˆ†æ
feature_importance = pd.DataFrame({
    'Feature': features,
    'Coefficient': model.coef_,
    'Abs_Coefficient': np.abs(model.coef_),
    'Impact_Percentage': np.abs(model.coef_) / np.sum(np.abs(model.coef_)) * 100
}).sort_values('Abs_Coefficient', ascending=False)

print(f"\nğŸ” ç‰¹å¾µé‡è¦æ€§æ’åï¼š")
print(feature_importance[['Feature', 'Coefficient', 'Impact_Percentage']])

# 7ï¸âƒ£ å•†æ¥­è§£é‡‹
print(f"\nğŸ’¼ å•†æ¥­æ„ç¾©è§£é‡‹ï¼š")
for i, row in feature_importance.iterrows():
    feature = row['Feature']
    coef = row['Coefficient']
    
    if feature == 'size':
        print(f"ğŸ“ æˆ¿å±‹é¢ç©ï¼šæ¯å¢åŠ 1å¹³æ–¹è‹±å°ºï¼Œæˆ¿åƒ¹å¢åŠ  ${coef:.0f}")
    elif feature == 'bedrooms':
        print(f"ğŸ›ï¸ è‡¥å®¤æ•¸é‡ï¼šæ¯å¢åŠ 1é–“è‡¥å®¤ï¼Œæˆ¿åƒ¹å¢åŠ  ${coef:,.0f}")
    elif feature == 'bathrooms':
        print(f"ğŸš¿ æµ´å®¤æ•¸é‡ï¼šæ¯å¢åŠ 1é–“æµ´å®¤ï¼Œæˆ¿åƒ¹å¢åŠ  ${coef:,.0f}")
    elif feature == 'age':
        print(f"ğŸ“… æˆ¿å±‹å¹´é½¡ï¼šæ¯å¢åŠ 1å¹´ï¼Œæˆ¿åƒ¹æ¸›å°‘ ${abs(coef):,.0f}")
    elif feature == 'location_score':
        print(f"ğŸ“ ä½ç½®è©•åˆ†ï¼šæ¯æå‡1åˆ†ï¼Œæˆ¿åƒ¹å¢åŠ  ${coef:,.0f}")
    elif feature == 'garage':
        print(f"ğŸš— è»Šåº«æ•¸é‡ï¼šæ¯å¢åŠ 1å€‹è»Šåº«ï¼Œæˆ¿åƒ¹å¢åŠ  ${coef:,.0f}")

# 8ï¸âƒ£ å¯¦éš›é æ¸¬ç¤ºä¾‹
print(f"\nğŸ  å¯¦éš›é æ¸¬ç¤ºä¾‹ï¼š")
sample_house = {
    'size': 2000,
    'bedrooms': 3,
    'bathrooms': 2,
    'age': 10,
    'location_score': 7.5,
    'garage': 2
}

sample_df = pd.DataFrame([sample_house])
predicted_price = model.predict(sample_df)[0]

print(f"æˆ¿å±‹ç‰¹å¾µï¼š")
for key, value in sample_house.items():
    print(f"  {key}: {value}")
print(f"é æ¸¬æˆ¿åƒ¹ï¼š${predicted_price:,.0f}")

# 9ï¸âƒ£ è¦–è¦ºåŒ–åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# é æ¸¬ vs å¯¦éš›
axes[0,0].scatter(y_test, y_pred_test, alpha=0.6)
axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0,0].set_xlabel('å¯¦éš›æˆ¿åƒ¹')
axes[0,0].set_ylabel('é æ¸¬æˆ¿åƒ¹')
axes[0,0].set_title(f'é æ¸¬ vs å¯¦éš› (RÂ² = {test_r2:.3f})')

# æ®˜å·®åœ–
residuals = y_test - y_pred_test
axes[0,1].scatter(y_pred_test, residuals, alpha=0.6)
axes[0,1].axhline(y=0, color='r', linestyle='--')
axes[0,1].set_xlabel('é æ¸¬æˆ¿åƒ¹')
axes[0,1].set_ylabel('æ®˜å·®')
axes[0,1].set_title('æ®˜å·®åˆ†æ')

# ç‰¹å¾µé‡è¦æ€§
axes[1,0].barh(feature_importance['Feature'], feature_importance['Abs_Coefficient'])
axes[1,0].set_xlabel('ä¿‚æ•¸çµ•å°å€¼')
axes[1,0].set_title('ç‰¹å¾µé‡è¦æ€§')

# é æ¸¬èª¤å·®åˆ†å¸ƒ
axes[1,1].hist(residuals, bins=30, alpha=0.7)
axes[1,1].set_xlabel('æ®˜å·®')
axes[1,1].set_ylabel('é »ç‡')
axes[1,1].set_title('æ®˜å·®åˆ†å¸ƒ')

plt.tight_layout()
plt.show()

# ğŸ”Ÿ æ¨¡å‹è¨ºæ–·æª¢æŸ¥
print(f"\nğŸ” æ¨¡å‹è¨ºæ–·æª¢æŸ¥ï¼š")

# æª¢æŸ¥æ®˜å·®çš„æ­£æ…‹æ€§
from scipy import stats
_, normality_p = stats.shapiro(residuals[:100])  # é™åˆ¶æ¨£æœ¬æ•¸
print(f"æ®˜å·®æ­£æ…‹æ€§æª¢å®š på€¼ï¼š{normality_p:.4f}")
if normality_p > 0.05:
    print("âœ… æ®˜å·®ç¬¦åˆæ­£æ…‹åˆ†å¸ƒ")
else:
    print("âš ï¸ æ®˜å·®ä¸ç¬¦åˆæ­£æ…‹åˆ†å¸ƒ")

# æª¢æŸ¥ç·šæ€§é—œä¿‚
correlation_matrix = X.corr()
print(f"\nç‰¹å¾µé–“ç›¸é—œæ€§ï¼ˆæª¢æŸ¥å¤šé‡å…±ç·šæ€§ï¼‰ï¼š")
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr = correlation_matrix.iloc[i, j]
        if abs(corr) > 0.7:
            high_corr_pairs.append((correlation_matrix.columns[i], 
                                  correlation_matrix.columns[j], corr))

if high_corr_pairs:
    print("âš ï¸ ç™¼ç¾é«˜ç›¸é—œç‰¹å¾µå°ï¼š")
    for pair in high_corr_pairs:
        print(f"  {pair[0]} - {pair[1]}: {pair[2]:.3f}")
else:
    print("âœ… ç„¡åš´é‡å¤šé‡å…±ç·šæ€§å•é¡Œ")
```

## å››ã€å­¸ç¿’é€²éšæŒ‡å—

### (ä¸€) å­¸ç¿’éšæ®µè¦åŠƒ

**ğŸ¯ åˆå­¸è€…è·¯å¾‘ï¼ˆ4-6é€±ï¼‰**

**ç¬¬1é€±ï¼šç’°å¢ƒè¨­ç½®èˆ‡åŸºç¤å·¥å…·**
- ğŸ› ï¸ **ç’°å¢ƒæº–å‚™**ï¼šå®‰è£ Anacondaã€Jupyter Notebook
- ğŸ¼ **Pandas å…¥é–€**ï¼šDataFrame æ“ä½œã€è³‡æ–™è®€å–
- ğŸ“Š **åŸºç¤è¦–è¦ºåŒ–**ï¼šMatplotlib åŸºæœ¬åœ–è¡¨
- ğŸ’¡ **å¯¦ä½œ**ï¼šåˆ†æä¸€å€‹ç°¡å–®çš„ CSV æª”æ¡ˆ

**ç¬¬2é€±ï¼šè³‡æ–™æ¢ç´¢èˆ‡æ¸…ç†**
- ğŸ” **è³‡æ–™æ¢ç´¢**ï¼š`describe()`, `info()`, ç¼ºå¤±å€¼æª¢æŸ¥
- ğŸ§¹ **è³‡æ–™æ¸…ç†**ï¼šè™•ç†ç¼ºå¤±å€¼ã€é‡è¤‡å€¼ã€ç•°å¸¸å€¼
- ğŸ“ˆ **æè¿°çµ±è¨ˆ**ï¼šå¹³å‡å€¼ã€ä¸­ä½æ•¸ã€æ¨™æº–å·®çš„è¨ˆç®—èˆ‡è§£é‡‹
- ğŸ’¡ **å¯¦ä½œ**ï¼šæ¸…ç†ä¸€å€‹ã€Œé«’ã€è³‡æ–™é›†

**ç¬¬3é€±ï¼šé€²éšè³‡æ–™è™•ç†**
- ğŸ”„ **è³‡æ–™é‡å¡‘**ï¼š`pivot()`, `melt()`, `groupby()`
- ğŸ”— **è³‡æ–™åˆä½µ**ï¼šä¸åŒé¡å‹çš„ join æ“ä½œ
- ğŸ¨ **é€²éšè¦–è¦ºåŒ–**ï¼šSeaborn çµ±è¨ˆåœ–è¡¨
- ğŸ’¡ **å¯¦ä½œ**ï¼šå¤šè¡¨æ ¼è³‡æ–™æ•´åˆåˆ†æ

**ç¬¬4é€±ï¼šçµ±è¨ˆåˆ†æåŸºç¤**
- ğŸ“Š **å‡è¨­æª¢å®š**ï¼štæª¢å®šã€ANOVA çš„å¯¦éš›æ‡‰ç”¨
- ğŸ“ˆ **ç›¸é—œåˆ†æ**ï¼šPearsonã€Spearman ç›¸é—œä¿‚æ•¸
- ğŸŒ **äº’å‹•è¦–è¦ºåŒ–**ï¼šPlotly åŸºç¤åœ–è¡¨
- ğŸ’¡ **å¯¦ä½œ**ï¼šA/B æ¸¬è©¦æ•ˆæœåˆ†æ

**ç¬¬5-6é€±ï¼šè¿´æ­¸åˆ†æèˆ‡å°ˆæ¡ˆ**
- ğŸ¯ **è¿´æ­¸å»ºæ¨¡**ï¼šç·šæ€§è¿´æ­¸ã€æ¨¡å‹è©•ä¼°
- ğŸ” **æ¨¡å‹è¨ºæ–·**ï¼šæ®˜å·®åˆ†æã€å‡è¨­æª¢é©—
- ğŸ“‹ **å®Œæ•´å°ˆæ¡ˆ**ï¼šå¾è³‡æ–™æ”¶é›†åˆ°çµè«–å ±å‘Š
- ğŸ’¡ **å¯¦ä½œ**ï¼šé æ¸¬æ¨¡å‹å»ºç«‹èˆ‡é©—è­‰

### (äºŒ) å¯¦æˆ°å°ˆæ¡ˆæ¨è–¦

**ğŸ¥‰ åˆç´šå°ˆæ¡ˆï¼šé›¶å”®éŠ·å”®åˆ†æ**
```
å°ˆæ¡ˆç›®æ¨™ï¼šåˆ†æè¶…å¸‚åŠå¹´éŠ·å”®è³‡æ–™
è³‡æ–™è¦æ¨¡ï¼š1000-5000 ç­†äº¤æ˜“è¨˜éŒ„
æŠ€èƒ½é‡é»ï¼šPandas æ“ä½œã€åŸºç¤çµ±è¨ˆã€Matplotlib
é æœŸç”¢å‡ºï¼š
- ğŸ“Š æœˆåº¦éŠ·å”®è¶¨å‹¢åœ–
- ğŸ† TOP 10 ç”¢å“æ’è¡Œæ¦œ
- ğŸ“ˆ å­£ç¯€æ€§åˆ†æå ±å‘Š
- ğŸ’° ç‡Ÿæ”¶ä¾†æºåˆ†æ
å­¸ç¿’æ™‚é–“ï¼š1-2 é€±
```

**ğŸ¥ˆ ä¸­ç´šå°ˆæ¡ˆï¼šå®¢æˆ¶è¡Œç‚ºåˆ†æ**
```
å°ˆæ¡ˆç›®æ¨™ï¼šé›»å•†ç¶²ç«™ç”¨æˆ¶è¡Œç‚ºåˆ†æ
è³‡æ–™è¦æ¨¡ï¼š10000+ ç”¨æˆ¶è¡Œç‚ºè¨˜éŒ„
æŠ€èƒ½é‡é»ï¼šè³‡æ–™æ¸…ç†ã€çµ±è¨ˆæª¢å®šã€Seaborn
é æœŸç”¢å‡ºï¼š
- ğŸ‘¥ å®¢æˆ¶åˆ†ç¾¤åˆ†æ
- ğŸ”„ æµå¤±å®¢æˆ¶è­˜åˆ¥
- ğŸ“± è³¼è²·è·¯å¾‘åˆ†æ  
- ğŸ¯ å€‹äººåŒ–æ¨è–¦ç­–ç•¥
å­¸ç¿’æ™‚é–“ï¼š3-4 é€±
```

**ğŸ¥‡ é«˜ç´šå°ˆæ¡ˆï¼šç‡Ÿé‹ç¸¾æ•ˆç›£æ§ç³»çµ±**
```
å°ˆæ¡ˆç›®æ¨™ï¼šå»ºç«‹å³æ™‚ç‡Ÿé‹ç›£æ§å„€è¡¨æ¿
è³‡æ–™è¦æ¨¡ï¼šå¤šæ•¸æ“šæºæ•´åˆ
æŠ€èƒ½é‡é»ï¼šå®Œæ•´è³‡æ–™ç§‘å­¸æµç¨‹ã€Plotlyã€é æ¸¬å»ºæ¨¡
é æœŸç”¢å‡ºï¼š
- ğŸ“Š å³æ™‚KPIç›£æ§é¢æ¿
- ğŸ”® éŠ·é‡é æ¸¬æ¨¡å‹
- ğŸš¨ ç•°å¸¸æª¢æ¸¬ç³»çµ±
- ğŸ“± è¡Œå‹•ç«¯å‹å–„ä»‹é¢
å­¸ç¿’æ™‚é–“ï¼š6-8 é€±
```

### (ä¸‰) é€²éšæŠ€èƒ½ç™¼å±•

**ğŸ”¬ çµ±è¨ˆå­¸é€²éš**
- **è²è‘‰æ–¯çµ±è¨ˆ**ï¼šä¸ç¢ºå®šæ€§é‡åŒ–ã€å…ˆé©—çŸ¥è­˜æ•´åˆ
- **æ™‚é–“åºåˆ—åˆ†æ**ï¼šè¶¨å‹¢åˆ†æã€å­£ç¯€æ€§åˆ†è§£ã€ARIMAæ¨¡å‹
- **å­˜æ´»åˆ†æ**ï¼šå®¢æˆ¶ç”Ÿå‘½é€±æœŸã€æµå¤±æ™‚é–“é æ¸¬
- **å› æœæ¨è«–**ï¼šA/Bæ¸¬è©¦è¨­è¨ˆã€å› æœé—œä¿‚è­˜åˆ¥

**ğŸ¤– æ©Ÿå™¨å­¸ç¿’å…¥é–€**
- **ç›£ç£å­¸ç¿’**ï¼šæ±ºç­–æ¨¹ã€éš¨æ©Ÿæ£®æ—ã€æ”¯æ´å‘é‡æ©Ÿ
- **éç›£ç£å­¸ç¿’**ï¼šK-meansèšé¡ã€PCAé™ç¶­
- **æ¨¡å‹é¸æ“‡**ï¼šäº¤å‰é©—è­‰ã€ç¶²æ ¼æœå°‹ã€æ¨¡å‹æ¯”è¼ƒ
- **ç‰¹å¾µå·¥ç¨‹**ï¼šç‰¹å¾µé¸æ“‡ã€å‰µé€ ã€è½‰æ›

**ğŸ“Š é€²éšè¦–è¦ºåŒ–**
- **Dashboard é–‹ç™¼**ï¼šStreamlitã€Dash äº’å‹•æ‡‰ç”¨
- **åœ°ç†è³‡æ–™è¦–è¦ºåŒ–**ï¼šFolium åœ°åœ–ã€Geopandas
- **å¤§æ•¸æ“šè¦–è¦ºåŒ–**ï¼šDatashaderã€Bokeh
- **å•†æ¥­æ™ºæ…§å·¥å…·**ï¼šTableauã€Power BI æ•´åˆ

### (å››) å­¸ç¿’è³‡æºèˆ‡å·¥å…·

**ğŸ“š æ¨è–¦æ›¸ç±**
- **å…¥é–€ç´š**ï¼šã€ŠPythonè³‡æ–™ç§‘å­¸æ‰‹å†Šã€‹- Jake VanderPlas
- **é€²éšç´š**ï¼šã€Šçµ±è¨ˆå­¸ç¿’å°è«–ã€‹- Gareth James
- **å¯¦å‹™ç´š**ï¼šã€Šæ•¸æ“šç§‘å­¸å¯¦æˆ°ã€‹- Joel Grus

**ğŸ’» ç·šä¸Šå¹³å°**
- **å…è²»è³‡æº**ï¼šKaggle Learnã€YouTube æ•™å­¸é »é“
- **ä»˜è²»èª²ç¨‹**ï¼šCourseraã€edXã€DataCamp
- **å¯¦ä½œç·´ç¿’**ï¼šKaggle Competitionsã€GitHub Projects

**ğŸ› ï¸ é–‹ç™¼å·¥å…·**
- **IDEç’°å¢ƒ**ï¼šJupyter Notebookã€VS Codeã€PyCharm
- **é›²ç«¯å¹³å°**ï¼šGoogle Colabã€Kaggle Kernelsã€AWS SageMaker
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šGitã€GitHubã€è³‡æ–™ç‰ˆæœ¬æ§åˆ¶ DVC

**ğŸŒ è³‡æ–™ä¾†æº**
- **å…¬é–‹è³‡æ–™é›†**ï¼šKaggleã€UCI ML Repositoryã€æ”¿åºœé–‹æ”¾è³‡æ–™
- **APIè³‡æ–™**ï¼šTwitter APIã€è‚¡ç¥¨å¸‚å ´ APIã€å¤©æ°£è³‡æ–™ API
- **ç¶²è·¯çˆ¬èŸ²**ï¼šBeautifulSoupã€Scrapyã€Selenium

### (äº”) è·æ¶¯ç™¼å±•æ–¹å‘

**ğŸ“ˆ è³‡æ–™åˆ†æå¸«è·¯å¾‘**
- **åˆç´š**ï¼šè³‡æ–™æ¸…ç†ã€åŸºç¤åˆ†æã€å ±è¡¨è£½ä½œ
- **ä¸­ç´š**ï¼šå•†æ¥­æ´å¯Ÿã€é æ¸¬æ¨¡å‹ã€A/Bæ¸¬è©¦
- **é«˜ç´š**ï¼šç­–ç•¥åˆ†æã€ç”¢å“åˆ†æã€ç‡Ÿé‹å„ªåŒ–

**ğŸ”¬ è³‡æ–™ç§‘å­¸å®¶è·¯å¾‘**
- **æŠ€èƒ½éœ€æ±‚**ï¼šçµ±è¨ˆå­¸ã€æ©Ÿå™¨å­¸ç¿’ã€ç¨‹å¼è¨­è¨ˆ
- **å·¥ä½œå…§å®¹**ï¼šæ¼”ç®—æ³•é–‹ç™¼ã€æ¨¡å‹éƒ¨ç½²ã€ç ”ç©¶å‰µæ–°
- **ç™¼å±•æ–¹å‘**ï¼šæ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ã€é›»è…¦è¦–è¦º

**ğŸ’¼ å•†æ¥­åˆ†æå¸«è·¯å¾‘**
- **æ ¸å¿ƒèƒ½åŠ›**ï¼šå•†æ¥­ç†è§£ã€æºé€šæŠ€å·§ã€ç­–ç•¥æ€ç¶­
- **å·¥ä½œé‡é»**ï¼šå¸‚å ´åˆ†æã€ç«¶çˆ­æƒ…å ±ã€æŠ•è³‡è©•ä¼°
- **åƒ¹å€¼å‰µé€ **ï¼šå•†æ¥­æ±ºç­–æ”¯æ´ã€æµç¨‹å„ªåŒ–ã€é¢¨éšªç®¡ç†

è¨˜ä½ï¼Œ**å¯¦ä½œæ˜¯æœ€å¥½çš„å­¸ç¿’æ–¹å¼**ï¼ğŸš€ å¾å°å°ˆæ¡ˆé–‹å§‹ï¼Œé€æ­¥å»ºç«‹ä¿¡å¿ƒå’ŒæŠ€èƒ½ï¼Œæ¯å€‹å°ˆæ¡ˆéƒ½æ˜¯å‘å°ˆæ¥­è³‡æ–™åˆ†æå¸«é‚é€²çš„ä¸€æ­¥ã€‚
