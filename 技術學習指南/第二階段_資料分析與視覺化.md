# 貳、資料分析與視覺化

## 一、Pandas 資料分析基礎

### (一) Pandas 基礎概念

**什麼是 Pandas？**
- 🐼 Pandas 是 Python 中最重要的資料分析庫
- 📊 主要用於處理結構化資料（如 Excel 表格）
- 🔄 提供資料清理、轉換、分析的完整工具
- 💡 **想像成**：把 Excel 的功能用程式碼實現

**核心資料結構比較**

| 結構 | 說明 | 類比 | 使用時機 |
|------|------|------|----------|
| Series | 一維資料（單一欄位） | Excel 的一欄 | 處理單一變數 |
| DataFrame | 二維資料（多欄位表格） | 完整的 Excel 表格 | 大部分資料分析 |

### (二) 建立和讀取資料

```python
import pandas as pd
import numpy as np

# 1. 手動創建 DataFrame - 適合練習和小型資料
df = pd.DataFrame({
    'A': [1, 2, 3],           # 數值欄位
    'B': ['a', 'b', 'c']      # 文字欄位
})
print("手動建立的 DataFrame:")
print(df)

# 2. 從檔案讀取資料 - 實際工作中最常用
df_csv = pd.read_csv('data.csv')        # 讀取 CSV 檔案
df_excel = pd.read_excel('data.xlsx')   # 讀取 Excel 檔案

# 讀取時常用參數
df_csv = pd.read_csv('data.csv', 
                     encoding='utf-8',     # 中文編碼
                     index_col=0,          # 第一欄當作索引
                     na_values=['', '無']) # 指定空值的表示方式

# 💡 實用技巧：讀取大檔案時先看前幾行
df_sample = pd.read_csv('large_file.csv', nrows=5)  # 只讀取前5行
print("檔案預覽:")
print(df_sample)
```

### (三) 資料探索 - 了解你的資料

```python
# 📋 基本資訊查看 - 第一步一定要做！
df.info()          # 📊 查看資料框架整體資訊（欄位類型、記憶體使用）
df.describe()      # 📈 數值型欄位的統計摘要（平均、最大最小值等）
df.head()          # 👀 查看前5行（預設）
df.head(10)        # 👀 查看前10行
df.tail()          # 👀 查看最後5行
df.shape           # 📏 查看維度 (行數, 欄數)

# 💡 為什麼要先探索？
# - 了解資料大小和結構
# - 發現缺失值和異常值
# - 確認資料類型是否正確

print(f"資料集有 {df.shape[0]} 行，{df.shape[1]} 欄")
print(f"欄位名稱：{list(df.columns)}")
```

### (四) 資料選取和過濾

```python
# 🎯 選取資料的不同方法

# 1. 選取欄位
df['A']              # 選取單一欄位 → 回傳 Series
df[['A', 'B']]       # 選取多個欄位 → 回傳 DataFrame
                     # 💡 注意：雙層中括號！

# 2. 選取行（列）
df.loc[0]            # 按標籤選取第一行
df.loc[0:2]          # 選取第0到第2行
df.iloc[0]           # 按位置選取第一行
df.iloc[0:3]         # 選取前3行（不包含第3行）

# 3. 同時選取行和列
df.loc[0:2, 'A']          # 選取第0-2行的A欄
df.iloc[0:3, 0:2]         # 選取前3行、前2欄

# 🔍 條件過濾 - 非常實用！
df[df['A'] > 2]           # 條件過濾：A欄大於2的所有行
df[df['B'] == 'a']        # 文字條件：B欄等於'a'的行
df[(df['A'] > 1) & (df['A'] < 3)]  # 複合條件：且
df[(df['A'] > 2) | (df['B'] == 'a')]  # 複合條件：或

# 💡 query 方法 - 更直觀的寫法
df.query('A > 2')         # 等同於 df[df['A'] > 2]
df.query('A > 1 and A < 3')  # 複合條件的簡潔寫法
```

### (五) 資料排序和統計

```python
# 📊 排序操作
df.sort_values('A', ascending=False)  # 按A欄降序排序
df.sort_values(['A', 'B'])           # 多欄位排序：先按A，再按B
df.sort_index()                      # 按索引排序

# 📈 基礎統計 - 了解資料分布
df['A'].mean()      # 平均值 - 了解資料中心趨勢
df['A'].median()    # 中位數 - 不受極端值影響的中心值
df['A'].mode()      # 眾數 - 最常出現的值
df['A'].std()       # 標準差 - 資料分散程度
df['A'].var()       # 變異數 - 標準差的平方
df['A'].min()       # 最小值
df['A'].max()       # 最大值
df['A'].count()     # 非空值個數

# 💡 實用技巧：一次看所有統計
df.describe()       # 自動計算所有數值欄位的統計摘要
```

### (六) 資料清理與預處理

**為什麼需要資料清理？**
- 🧹 真實世界的資料通常很「髒」：有缺失值、重複、錯誤
- 💡 **80/20 法則**：資料科學家 80% 時間在清理資料，20% 在分析
- 🎯 目標：讓資料變得「乾淨」，適合分析

#### 1. 處理缺失值（Missing Values）

```python
# 🔍 檢查缺失值 - 先了解問題有多嚴重
df.isna().sum()           # 每欄的缺失值數量
df.isna().sum() / len(df) # 每欄的缺失值比例（百分比）

# 視覺化缺失值分布
import seaborn as sns
import matplotlib.pyplot as plt
sns.heatmap(df.isna(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('缺失值分布圖')
plt.show()

# 💊 處理缺失值的策略
# 策略1：填充（適合少量缺失）
df.fillna(0)                    # 用0填充所有缺失值
df.fillna(method='ffill')       # 用前一個值填充
df.fillna(method='bfill')       # 用後一個值填充
df['A'].fillna(df['A'].mean())  # 用平均值填充數值欄位
df['B'].fillna(df['B'].mode()[0])  # 用眾數填充類別欄位

# 策略2：刪除（適合大量缺失或不重要的資料）
df.dropna()                     # 刪除包含任何缺失值的行
df.dropna(subset=['A'])         # 只在A欄有缺失時才刪除行
df.dropna(thresh=2)             # 至少要有2個非空值才保留行

# 💡 選擇策略的原則：
# - 缺失 < 5%：可以刪除
# - 缺失 5-15%：考慮填充
# - 缺失 > 15%：謹慎處理，可能需要重新收集資料
```

#### 2. 處理重複值

```python
# 🔍 檢查重複值
df.duplicated().sum()        # 重複行的數量
print(f"共有 {df.duplicated().sum()} 行重複資料")

# 查看重複的資料
duplicate_rows = df[df.duplicated()]
print("重複的資料：")
print(duplicate_rows)

# 🧹 刪除重複值
df_clean = df.drop_duplicates()         # 刪除重複行
df_clean = df.drop_duplicates(subset=['A'])  # 只根據A欄判斷是否重複
df_clean = df.drop_duplicates(keep='last')   # 保留最後一個重複值

print(f"清理前：{len(df)} 行")
print(f"清理後：{len(df_clean)} 行")
print(f"刪除了 {len(df) - len(df_clean)} 行重複資料")
```

#### 3. 資料類型轉換

```python
# 🔧 為什麼要轉換資料類型？
# - 節省記憶體空間
# - 確保計算的正確性
# - 某些操作需要特定類型

# 檢查目前的資料類型
print("目前的資料類型：")
print(df.dtypes)

# 💱 常見轉換
df['A'] = df['A'].astype('int64')      # 轉換為整數
df['B'] = df['B'].astype('category')   # 轉換為類別型（節省記憶體）

# 安全轉換（處理轉換失敗的情況）
df['C'] = pd.to_numeric(df['C'], errors='coerce')  # 無法轉換的變成 NaN
df['date'] = pd.to_datetime(df['date'], errors='coerce')  # 轉換為日期

# 💡 類別型資料的好處
# - 記憶體使用量大幅減少（特別是重複值多的欄位）
# - 自動排序和群組化
df['grade'] = df['grade'].astype('category')
print(f"轉換前記憶體使用：{df.memory_usage().sum()} bytes")
```

#### 4. 異常值檢測和處理

```python
# 🎯 什麼是異常值？
# - 與其他觀測值明顯不同的數據點
# - 可能是：測量錯誤、資料輸入錯誤、或真實的極端情況

# 📊 使用 IQR 方法檢測異常值（最常用）
def detect_outliers_iqr(df, column):
    """
    使用四分位距（IQR）方法檢測異常值
    
    原理：
    - Q1: 第25百分位數
    - Q3: 第75百分位數  
    - IQR = Q3 - Q1
    - 異常值: < Q1-1.5*IQR 或 > Q3+1.5*IQR
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    print(f"{column} 欄位異常值檢測結果：")
    print(f"下界：{lower_bound:.2f}, 上界：{upper_bound:.2f}")
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"發現 {len(outliers)} 個異常值")
    
    return outliers

# 使用函數
outliers = detect_outliers_iqr(df, 'A')

# 🔧 處理異常值的方法
def remove_outliers(df, column):
    """移除異常值"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

def cap_outliers(df, column):
    """限制異常值（比移除更保守）"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
    return df

# 💡 選擇處理方式：
# - 移除：確定是錯誤資料時
# - 限制：可能是真實極端值時
# - 保留：需要研究極端情況時
```

#### 5. 資料標準化和正規化

```python
# 🎯 為什麼需要標準化？
# - 不同欄位的尺度差異很大（如年齡 vs 收入）
# - 機器學習算法對尺度敏感
# - 讓所有特徵在同一尺度上比較

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# 準備數據（只對數值欄位標準化）
numeric_columns = df.select_dtypes(include=[np.number]).columns

# 方法1：標準化 (Z-score normalization)
# 轉換後：平均值=0，標準差=1
scaler = StandardScaler()
df_standardized = df.copy()
df_standardized[numeric_columns] = scaler.fit_transform(df[numeric_columns])

print("標準化後的統計：")
print(df_standardized[numeric_columns].describe())

# 方法2：最小-最大正規化
# 轉換後：所有值在 0-1 之間
minmax_scaler = MinMaxScaler()
df_normalized = df.copy()
df_normalized[numeric_columns] = minmax_scaler.fit_transform(df[numeric_columns])

# 方法3：穩健標準化 (適用於有異常值的資料)
# 使用中位數和四分位距，對異常值不敏感
robust_scaler = RobustScaler()
df_robust = df.copy()
df_robust[numeric_columns] = robust_scaler.fit_transform(df[numeric_columns])

# 💡 選擇標準化方法的指南：
# - StandardScaler: 資料呈常態分布
# - MinMaxScaler: 需要固定範圍(0-1)
# - RobustScaler: 資料有異常值
```

### (七) 資料合併與重塑

**為什麼需要合併和重塑資料？**
- 🔗 實際工作中，資料通常分散在多個檔案或表格中
- 📊 需要將不同來源的資料整合分析
- 🔄 不同的分析需要不同的資料格式

#### 1. 資料合併（Concatenation & Merging）

```python
# 🔗 垂直合併（Concatenation）- 把表格上下接起來
# 適用：相同欄位的不同資料集
df1 = pd.DataFrame({'A': [1, 2], 'B': ['x', 'y']})
df2 = pd.DataFrame({'A': [3, 4], 'B': ['z', 'w']})

df_vertical = pd.concat([df1, df2])              # 垂直合併
df_vertical = pd.concat([df1, df2], ignore_index=True)  # 重新編號索引

print("垂直合併結果：")
print(df_vertical)

# 🔗 水平合併（Concatenation）- 把表格左右接起來  
# 適用：相同觀測單位的不同特徵
df3 = pd.DataFrame({'C': [10, 20], 'D': [30, 40]})
df_horizontal = pd.concat([df1, df3], axis=1)    # axis=1 表示水平合併

print("水平合併結果：")
print(df_horizontal)

# 🔗 關聯式合併（Merge）- 像 SQL 的 JOIN
# 根據共同欄位合併不同表格

# 準備示例資料
customers = pd.DataFrame({
    'customer_id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Charlie'],
    'city': ['台北', '高雄', '台中']
})

orders = pd.DataFrame({
    'order_id': [101, 102, 103, 104],
    'customer_id': [1, 2, 1, 3],
    'amount': [1000, 1500, 800, 2000]
})

# 💡 不同的合併方式
# Inner Join: 只保留兩邊都有的記錄
inner_merge = pd.merge(customers, orders, on='customer_id', how='inner')
print("Inner Join - 只保留兩邊都有的客戶：")
print(inner_merge)

# Left Join: 保留左表所有記錄
left_merge = pd.merge(customers, orders, on='customer_id', how='left')
print("Left Join - 保留所有客戶：")
print(left_merge)

# Right Join: 保留右表所有記錄  
right_merge = pd.merge(customers, orders, on='customer_id', how='right')

# Outer Join: 保留兩邊所有記錄
outer_merge = pd.merge(customers, orders, on='customer_id', how='outer')

# 💡 合併的實際應用場景：
# - 客戶資料 + 訂單資料
# - 產品資料 + 銷售資料
# - 員工資料 + 薪資資料
```

#### 2. 資料重塑（Reshape）

```python
# 📊 樞紐表（Pivot Table）- 把長格式變寬格式
# 原始資料（長格式）
sales_data = pd.DataFrame({
    'date': ['2023-01', '2023-01', '2023-02', '2023-02'],
    'product': ['A', 'B', 'A', 'B'],
    'sales': [100, 150, 120, 180]
})

print("原始銷售資料（長格式）：")
print(sales_data)

# 轉換為樞紐表（寬格式）
pivot_table = sales_data.pivot(index='date', columns='product', values='sales')
print("樞紐表（寬格式）：")
print(pivot_table)

# 💡 樞紐表的用途：
# - 建立交叉分析表
# - 讓資料更易於閱讀
# - 適合製作報表

# 📊 融化（Melt）- 把寬格式變長格式
# 寬格式資料
wide_data = pd.DataFrame({
    'ID': [1, 2, 3],
    'Jan': [100, 200, 150],
    'Feb': [120, 180, 170],
    'Mar': [110, 220, 160]
})

print("寬格式資料：")
print(wide_data)

# 轉換為長格式
long_data = wide_data.melt(
    id_vars=['ID'],                    # 保持不變的欄位
    value_vars=['Jan', 'Feb', 'Mar'],  # 要融化的欄位
    var_name='Month',                  # 新的變數名稱欄位
    value_name='Sales'                 # 新的數值欄位
)

print("長格式資料：")
print(long_data)

# 💡 長格式 vs 寬格式的選擇：
# - 長格式：適合統計分析、視覺化
# - 寬格式：適合報表、人類閱讀
```

#### 3. 群組操作（Group Operations）

```python
# 🎯 群組分析 - 資料分析的核心技能
# 常見問題：「每個類別的平均值是多少？」

# 準備示例資料
employee_data = pd.DataFrame({
    'department': ['IT', 'HR', 'IT', 'HR', 'Finance', 'Finance'],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],
    'salary': [80000, 60000, 85000, 65000, 70000, 75000],
    'experience': [5, 3, 7, 4, 6, 8]
})

# 📊 基本群組統計
dept_stats = employee_data.groupby('department').mean()
print("各部門平均薪資和經驗：")
print(dept_stats)

# 📊 多重統計函數
dept_summary = employee_data.groupby('department').agg({
    'salary': ['mean', 'std', 'min', 'max'],    # 薪資的多項統計
    'experience': ['mean', 'count']              # 經驗的統計
})
print("各部門詳細統計：")
print(dept_summary)

# 📊 自定義聚合函數
def salary_range(series):
    return series.max() - series.min()

custom_agg = employee_data.groupby('department').agg({
    'salary': ['mean', salary_range],
    'name': 'count'  # 計算人數
})

# 💡 群組操作的實際應用：
# - 銷售分析：按地區、產品、時間分組
# - 客戶分析：按年齡段、消費等級分組  
# - 效能分析：按部門、職位分組

# 🔍 進階群組操作
# 多欄位分組
multi_group = employee_data.groupby(['department', 'experience > 5']).mean()

# 群組後篩選
high_salary_depts = employee_data.groupby('department').filter(
    lambda x: x['salary'].mean() > 70000
)
print("平均薪資超過70000的部門員工：")
print(high_salary_depts)
```

## 二、資料視覺化

### (一) 視覺化的重要性

**為什麼需要資料視覺化？**
- 👁️ **快速理解**：一圖勝千言，快速發現模式和趨勢
- 🧠 **直觀溝通**：向非技術人員說明發現更容易
- 🔍 **發現洞察**：視覺化能揭露數字表格中看不出的關係
- 🚨 **識別異常**：快速發現異常值和錯誤

**選擇合適圖表的指南**

| 目的 | 適合的圖表 | 使用時機 |
|------|----------|----------|
| 比較數值 | 長條圖、柱狀圖 | 比較不同類別的量 |
| 顯示趨勢 | 折線圖 | 時間序列資料 |
| 顯示分布 | 直方圖、箱形圖 | 了解資料分布形狀 |
| 顯示關係 | 散點圖 | 兩變數間的相關性 |
| 顯示組成 | 圓餅圖、堆疊圖 | 部分對整體的比例 |
| 顯示地理 | 地圖 | 空間分布資料 |

### (二) Matplotlib 基礎

**什麼是 Matplotlib？**
- 📊 Python 最基本、最重要的繪圖庫
- 🎨 類似 MATLAB 的繪圖語法
- 🔧 高度可客製化，但語法較複雜
- 🏗️ 其他繪圖庫的基礎（Seaborn、Pandas plot 都基於它）

```python
import matplotlib.pyplot as plt
import numpy as np

# 🎨 基本設定
plt.rcParams['font.family'] = 'Microsoft YaHei'  # 支援中文
plt.rcParams['figure.figsize'] = (10, 6)         # 預設圖片大小

# 📈 基本線圖 - 顯示趨勢
# 準備資料
x = np.linspace(0, 10, 100)  # 0到10之間的100個點
y = np.sin(x)                # 正弦函數

# 繪製圖表
plt.figure(figsize=(10, 6))           # 設定圖片大小
plt.plot(x, y, 'r-', label='sin(x)', linewidth=2)  # 紅色實線
plt.plot(x, np.cos(x), 'b--', label='cos(x)', linewidth=2)  # 藍色虛線

# 💡 美化圖表
plt.xlabel('X軸標籤', fontsize=12)    # X軸標籤
plt.ylabel('Y軸標籤', fontsize=12)    # Y軸標籤
plt.title('三角函數圖表', fontsize=14) # 圖表標題
plt.legend()                          # 顯示圖例
plt.grid(True, alpha=0.3)            # 顯示網格，透明度0.3
plt.show()                           # 顯示圖表

# 📊 散點圖 - 顯示兩變數關係
# 生成示例資料
np.random.seed(42)  # 固定隨機種子，確保結果可重現
x_scatter = np.random.randn(100)
y_scatter = 2 * x_scatter + np.random.randn(100)  # y與x有線性關係加上噪音

plt.figure(figsize=(8, 6))
plt.scatter(x_scatter, y_scatter, c='blue', alpha=0.6, s=50)
plt.xlabel('X 變數')
plt.ylabel('Y 變數') 
plt.title('散點圖：X 與 Y 的關係')
plt.grid(True, alpha=0.3)
plt.show()

# 📊 長條圖 - 比較類別資料
categories = ['產品A', '產品B', '產品C', '產品D']
values = [23, 45, 56, 78]

plt.figure(figsize=(8, 6))
bars = plt.bar(categories, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])

# 在長條上顯示數值
for bar, value in zip(bars, values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
             str(value), ha='center', va='bottom')

plt.xlabel('產品類別')
plt.ylabel('銷售量')
plt.title('各產品銷售量比較')
plt.xticks(rotation=45)  # 旋轉X軸標籤
plt.tight_layout()       # 自動調整佈局
plt.show()

# 📊 子圖 - 在一個圖中顯示多個圖表
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))

# 子圖1：線圖
ax1.plot(x, y)
ax1.set_title('線圖')
ax1.grid(True)

# 子圖2：散點圖  
ax2.scatter(x_scatter, y_scatter)
ax2.set_title('散點圖')

# 子圖3：長條圖
ax3.bar(categories, values)
ax3.set_title('長條圖')
ax3.tick_params(axis='x', rotation=45)

# 子圖4：直方圖
ax4.hist(y_scatter, bins=20, alpha=0.7)
ax4.set_title('直方圖')

plt.tight_layout()  # 避免子圖重疊
plt.show()

# 💡 Matplotlib 小技巧：
# - 使用 plt.style.use('seaborn') 快速美化
# - 用 plt.savefig('filename.png', dpi=300) 儲存高畫質圖片
# - 用 plt.close() 釋放記憶體
```

### (三) Seaborn 視覺化

**什麼是 Seaborn？**
- 🎨 基於 Matplotlib 的高級統計視覺化庫
- 📊 專門設計用於統計圖表
- 🌈 預設就很美觀，色彩搭配專業
- 📈 特別適合探索性資料分析（EDA）

```python
import seaborn as sns
import pandas as pd
import numpy as np

# 🎨 設定 Seaborn 風格
sns.set_style("whitegrid")        # 白色背景 + 網格
sns.set_palette("husl")           # 設定調色盤
sns.set_context("notebook")       # 設定大小（talk, paper, poster）

# 📊 準備示例資料
np.random.seed(42)
tips = sns.load_dataset("tips")   # Seaborn 內建資料集
print("Tips 資料集前5行：")
print(tips.head())

# 📈 分布圖 - 了解單一變數的分布
plt.figure(figsize=(12, 4))

# 1. 直方圖 + 密度曲線
plt.subplot(1, 3, 1)
sns.histplot(tips['total_bill'], kde=True)
plt.title('總帳單分布')

# 2. 只有密度曲線
plt.subplot(1, 3, 2)
sns.kdeplot(tips['total_bill'])
plt.title('總帳單密度圖')

# 3. 多組比較
plt.subplot(1, 3, 3)
sns.histplot(data=tips, x='total_bill', hue='time', alpha=0.7)
plt.title('按時間分組的總帳單分布')

plt.tight_layout()
plt.show()

# 📊 箱形圖 - 比較不同群組的分布
plt.figure(figsize=(12, 4))

# 1. 基本箱形圖
plt.subplot(1, 3, 1)
sns.boxplot(data=tips, x='day', y='total_bill')
plt.title('各天總帳單分布')
plt.xticks(rotation=45)

# 2. 分組箱形圖
plt.subplot(1, 3, 2)
sns.boxplot(data=tips, x='day', y='total_bill', hue='time')
plt.title('按時間分組的總帳單')
plt.xticks(rotation=45)

# 3. 小提琴圖 - 結合箱形圖和密度圖
plt.subplot(1, 3, 3)
sns.violinplot(data=tips, x='day', y='total_bill')
plt.title('小提琴圖')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# 📊 關係圖 - 探索變數間的關係
plt.figure(figsize=(15, 5))

# 1. 散點圖
plt.subplot(1, 3, 1)
sns.scatterplot(data=tips, x='total_bill', y='tip', hue='time', size='size')
plt.title('總帳單 vs 小費（按時間和人數分組）')

# 2. 回歸線圖
plt.subplot(1, 3, 2)
sns.regplot(data=tips, x='total_bill', y='tip', scatter_kws={'alpha':0.6})
plt.title('總帳單 vs 小費（含回歸線）')

# 3. 分組回歸線
plt.subplot(1, 3, 3)
sns.lmplot(data=tips, x='total_bill', y='tip', hue='smoker', height=4)
plt.title('按是否吸菸分組的回歸')

plt.tight_layout()
plt.show()

# 🔥 熱力圖 - 顯示相關性矩陣
# 計算相關性矩陣
correlation_matrix = tips.select_dtypes(include=[np.number]).corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, 
            annot=True,           # 顯示數值
            cmap='coolwarm',      # 色彩映射
            center=0,             # 以0為中心
            square=True,          # 正方形格子
            fmt='.2f')            # 數值格式
plt.title('變數相關性熱力圖')
plt.tight_layout()
plt.show()

# 📊 成對關係圖 - 一次看所有變數的關係
g = sns.pairplot(tips, 
                 hue='time',           # 按時間分色
                 diag_kind='kde',      # 對角線用密度圖
                 plot_kws={'alpha': 0.7})
g.fig.suptitle('所有變數成對關係圖', y=1.02)
plt.show()

# 💡 Seaborn 的優勢：
# 1. 語法簡潔：一行程式碼就能繪製複雜圖表
# 2. 統計功能：自動計算統計量（如回歸線、信賴區間）
# 3. 分組功能：用 hue, size, style 參數輕鬆分組
# 4. 美觀預設：不需調整就有專業外觀

# 🎯 常用的 Seaborn 圖表選擇指南：
# - 單變數分布：histplot, kdeplot, boxplot
# - 雙變數關係：scatterplot, regplot
# - 類別比較：boxplot, violinplot, barplot
# - 多變數探索：pairplot, heatmap
```

### (四) Plotly 互動視覺化

**什麼是 Plotly？**
- 🖱️ **互動式圖表**：支援縮放、平移、懸停顯示資訊
- 🌐 **網頁整合**：可輸出 HTML，嵌入網頁或報告
- 📱 **響應式設計**：在不同裝置上都能良好顯示
- 🎨 **專業外觀**：適合製作展示用的圖表

```python
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np

# 📊 準備示例資料
np.random.seed(42)
df_sales = pd.DataFrame({
    'month': pd.date_range('2023-01', periods=12, freq='M'),
    'product_A': np.random.randint(100, 200, 12),
    'product_B': np.random.randint(80, 180, 12),
    'region': ['North', 'South'] * 6
})

# 📈 散點圖 - 探索關係
fig_scatter = px.scatter(
    df_sales, 
    x='product_A', 
    y='product_B',
    color='region',                    # 按地區分色
    size='product_A',                  # 點的大小表示 product_A 銷量
    hover_data=['month'],              # 懸停時顯示月份
    title='產品A vs 產品B 銷量關係圖',
    labels={'product_A': '產品A銷量', 'product_B': '產品B銷量'}
)

# 💡 互動功能：
# - 懸停顯示詳細資訊
# - 點擊圖例隱藏/顯示類別
# - 縮放和平移
fig_scatter.show()

# 📈 時間序列線圖 - 顯示趨勢
# 重新整理資料為長格式
df_long = df_sales.melt(
    id_vars=['month', 'region'], 
    value_vars=['product_A', 'product_B'],
    var_name='product', 
    value_name='sales'
)

fig_line = px.line(
    df_long, 
    x='month', 
    y='sales',
    color='product',                   # 按產品分色
    line_dash='region',                # 按地區分線型
    title='2023年產品銷量趨勢',
    labels={'sales': '銷量', 'month': '月份'}
)

# 添加註釋
fig_line.add_annotation(
    x='2023-06',
    y=150,
    text="6月促銷活動",
    showarrow=True,
    arrowhead=2
)

fig_line.show()

# 📊 長條圖 - 比較類別
monthly_total = df_sales.groupby(df_sales['month'].dt.strftime('%Y-%m')).sum().reset_index()

fig_bar = px.bar(
    monthly_total, 
    x='month', 
    y=['product_A', 'product_B'],
    title='每月產品銷量比較',
    labels={'value': '銷量', 'month': '月份', 'variable': '產品'},
    color_discrete_map={'product_A': '#FF6B6B', 'product_B': '#4ECDC4'}
)

fig_bar.update_layout(
    xaxis_title="月份",
    yaxis_title="銷量",
    legend_title="產品類型"
)

fig_bar.show()

# 🥧 圓餅圖 - 顯示組成比例
total_sales = {
    'product_A': df_sales['product_A'].sum(),
    'product_B': df_sales['product_B'].sum()
}

fig_pie = px.pie(
    values=list(total_sales.values()), 
    names=list(total_sales.keys()),
    title='全年產品銷量占比',
    color_discrete_map={'product_A': '#FF6B6B', 'product_B': '#4ECDC4'}
)

# 自定義樣式
fig_pie.update_traces(
    textposition='inside', 
    textinfo='percent+label',
    hovertemplate='<b>%{label}</b><br>銷量: %{value}<br>占比: %{percent}<extra></extra>'
)

fig_pie.show()

# 🌐 3D 散點圖 - 多維度視覺化
df_3d = pd.DataFrame({
    'x': np.random.randn(100),
    'y': np.random.randn(100), 
    'z': np.random.randn(100),
    'category': np.random.choice(['A', 'B', 'C'], 100),
    'size': np.random.randint(10, 30, 100)
})

fig_3d = px.scatter_3d(
    df_3d, 
    x='x', 
    y='y', 
    z='z',
    color='category',
    size='size',
    title='3D 散點圖示例'
)

# 設定相機角度
fig_3d.update_layout(
    scene=dict(
        xaxis_title='X軸',
        yaxis_title='Y軸',
        zaxis_title='Z軸'
    )
)

fig_3d.show()

# 🔥 進階功能：子圖和儀表板
from plotly.subplots import make_subplots

# 建立子圖
fig_dashboard = make_subplots(
    rows=2, cols=2,
    subplot_titles=('時間趨勢', '相關性', '分布', '比較'),
    specs=[[{"secondary_y": True}, {"type": "scatter"}],
           [{"type": "histogram"}, {"type": "bar"}]]
)

# 添加不同類型的圖表
fig_dashboard.add_trace(
    go.Scatter(x=df_sales['month'], y=df_sales['product_A'], name='產品A'),
    row=1, col=1
)

fig_dashboard.add_trace(
    go.Scatter(x=df_sales['product_A'], y=df_sales['product_B'], 
               mode='markers', name='A vs B'),
    row=1, col=2
)

fig_dashboard.add_trace(
    go.Histogram(x=df_sales['product_A'], name='產品A分布'),
    row=2, col=1
)

fig_dashboard.add_trace(
    go.Bar(x=['產品A', '產品B'], 
           y=[df_sales['product_A'].mean(), df_sales['product_B'].mean()],
           name='平均銷量'),
    row=2, col=2
)

fig_dashboard.update_layout(
    title_text="銷量分析儀表板",
    showlegend=False,
    height=600
)

fig_dashboard.show()

# 💡 Plotly 的優勢：
# 1. 互動性：使用者可以探索資料
# 2. 專業外觀：適合商業報告
# 3. 易於分享：輸出 HTML 文件
# 4. 響應式：適應不同螢幕大小

# 🎯 使用建議：
# - 探索性分析：用 px (plotly express) 快速製圖
# - 客製化需求：用 go (graph objects) 精細控制
# - 報告展示：加入註釋和自定義樣式
# - 網頁整合：輸出 HTML 嵌入網站

# 💾 儲存圖表
# fig.write_html("my_plot.html")        # 儲存為 HTML
# fig.write_image("my_plot.png")        # 儲存為圖片（需安裝 kaleido）
```

## 三、統計分析

### (一) 統計分析的重要性

**為什麼需要統計分析？**
- 🔍 **發現模式**：從看似隨機的資料中找出規律
- 📊 **量化關係**：測量變數間的關聯強度
- 🎯 **驗證假設**：用數據證明或推翻我們的想法
- 🔮 **預測未來**：基於歷史資料預測趨勢

**統計分析的層次**

| 層次 | 目的 | 範例問題 | 使用方法 |
|------|------|----------|----------|
| 描述統計 | 總結資料特徵 | 平均年薪是多少？ | 平均值、中位數 |
| 推論統計 | 從樣本推論母體 | 新藥是否有效？ | 假設檢定 |
| 預測分析 | 預測未來趨勢 | 明年銷量會是多少？ | 迴歸分析 |

### (二) 描述統計

**描述統計的目標：用幾個數字總結整個資料集**

```python
import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

# 📊 準備示例資料 - 員工薪資資料
np.random.seed(42)
salaries = np.random.normal(60000, 15000, 1000)  # 平均60000，標準差15000
salaries = np.clip(salaries, 30000, 120000)      # 限制在合理範圍內

# 1️⃣ 中心趨勢 - 資料的「中心」在哪裡？
mean_salary = np.mean(salaries)         # 平均值：所有數的總和除以個數
median_salary = np.median(salaries)     # 中位數：排序後中間的數
mode_result = stats.mode(salaries)      # 眾數：最常出現的數

print("=== 中心趨勢分析 ===")
print(f"平均薪資：${mean_salary:,.0f}")
print(f"中位數薪資：${median_salary:,.0f}")
print(f"薪資眾數：${mode_result.mode[0]:,.0f}")

# 💡 什麼時候用哪個？
# - 平均值：資料分布對稱時使用
# - 中位數：有極端值時更穩健
# - 眾數：類別資料或想知道最常見值

# 視覺化中心趨勢
plt.figure(figsize=(10, 6))
plt.hist(salaries, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(mean_salary, color='red', linestyle='--', linewidth=2, label=f'平均值: ${mean_salary:,.0f}')
plt.axvline(median_salary, color='green', linestyle='--', linewidth=2, label=f'中位數: ${median_salary:,.0f}')
plt.xlabel('薪資 ($)')
plt.ylabel('員工數')
plt.title('員工薪資分布與中心趨勢')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# 2️⃣ 離散程度 - 資料散布得多廣？
variance = np.var(salaries)             # 變異數：平均偏差的平方
std_dev = np.std(salaries)              # 標準差：變異數的平方根
cv = std_dev / mean_salary              # 變異係數：標準化的離散程度
range_val = np.max(salaries) - np.min(salaries)  # 全距：最大值減最小值

print("\n=== 離散程度分析 ===")
print(f"標準差：${std_dev:,.0f}")
print(f"變異數：${variance:,.0f}")
print(f"變異係數：{cv:.2%}")
print(f"全距：${range_val:,.0f}")

# 💡 標準差的意義：
# - 約68%的資料在 平均值 ± 1個標準差 內
# - 約95%的資料在 平均值 ± 2個標準差 內
lower_1std = mean_salary - std_dev
upper_1std = mean_salary + std_dev
within_1std = np.sum((salaries >= lower_1std) & (salaries <= upper_1std)) / len(salaries)
print(f"在1個標準差內的資料比例：{within_1std:.1%}")

# 3️⃣ 分布形狀特徵
skewness = stats.skew(salaries)         # 偏度：分布是否對稱
kurtosis = stats.kurtosis(salaries)     # 峰度：分布的尖銳程度

print("\n=== 分布形狀特徵 ===")
print(f"偏度：{skewness:.3f}")
print(f"峰度：{kurtosis:.3f}")

# 💡 偏度解讀：
# - 偏度 = 0：對稱分布
# - 偏度 > 0：右偏（長尾在右邊）
# - 偏度 < 0：左偏（長尾在左邊）

if skewness > 0.5:
    skew_desc = "右偏（高薪員工較少）"
elif skewness < -0.5:
    skew_desc = "左偏（低薪員工較少）"
else:
    skew_desc = "近似對稱分布"

print(f"分布特徵：{skew_desc}")

# 4️⃣ 百分位數 - 資料的分位點
percentiles = np.percentile(salaries, [10, 25, 50, 75, 90])
q1, q3 = np.percentile(salaries, [25, 75])
iqr = q3 - q1  # 四分位距

print("\n=== 百分位數分析 ===")
print(f"第10百分位：${percentiles[0]:,.0f}")
print(f"第25百分位(Q1)：${percentiles[1]:,.0f}")
print(f"第50百分位(中位數)：${percentiles[2]:,.0f}")
print(f"第75百分位(Q3)：${percentiles[3]:,.0f}")
print(f"第90百分位：${percentiles[4]:,.0f}")
print(f"四分位距(IQR)：${iqr:,.0f}")

# 💡 百分位數的實際意義：
# - 第25百分位：25%的員工薪資低於此值
# - 第75百分位：75%的員工薪資低於此值
# - IQR：中間50%員工的薪資範圍

# 5️⃣ 一次性完整統計摘要
df_salaries = pd.DataFrame({'salary': salaries})
summary_stats = df_salaries.describe()

print("\n=== 完整統計摘要 ===")
print(summary_stats)

# 箱形圖視覺化
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.boxplot(salaries)
plt.ylabel('薪資 ($)')
plt.title('薪資箱形圖')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(salaries, bins=50, alpha=0.7, color='lightcoral')
plt.xlabel('薪資 ($)')
plt.ylabel('頻率')
plt.title('薪資分布直方圖')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### (三) 假設檢定

**什麼是假設檢定？**
- 🎯 **科學方法**：用資料證明或推翻我們的猜測
- 📊 **決策工具**：幫助做出基於證據的決定
- 🔍 **風險控制**：量化錯誤決策的機率

**假設檢定的基本概念**

| 概念 | 說明 | 範例 |
|------|------|------|
| 虛無假設(H₀) | 我們想要推翻的假設 | 新藥沒有效果 |
| 對立假設(H₁) | 我們想要證明的假設 | 新藥有效果 |
| p值 | 在H₀為真下，觀察到此結果的機率 | p = 0.03 |
| 顯著水準(α) | 我們能接受的錯誤機率 | α = 0.05 |

```python
import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 🧪 準備實驗資料
np.random.seed(42)

# 1️⃣ 常態性檢定 - 資料是否符合常態分布？
# 為什麼重要？很多統計方法假設資料是常態分布的

# 生成不同分布的資料
normal_data = np.random.normal(50, 10, 1000)      # 常態分布
uniform_data = np.random.uniform(30, 70, 1000)    # 均勻分布
skewed_data = np.random.exponential(2, 1000)      # 指數分布（右偏）

# Shapiro-Wilk 檢定（樣本數 < 5000 時使用）
def test_normality(data, name):
    statistic, p_value = stats.shapiro(data)
    print(f"\n=== {name} 常態性檢定 ===")
    print(f"Shapiro-Wilk 統計量：{statistic:.4f}")
    print(f"p值：{p_value:.6f}")
    
    if p_value > 0.05:
        print("✅ 結論：資料符合常態分布（p > 0.05）")
    else:
        print("❌ 結論：資料不符合常態分布（p ≤ 0.05）")
    
    return p_value

# 檢定不同資料
p_normal = test_normality(normal_data, "常態分布資料")
p_uniform = test_normality(uniform_data, "均勻分布資料")  
p_skewed = test_normality(skewed_data, "右偏分布資料")

# 視覺化比較
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].hist(normal_data, bins=50, alpha=0.7, color='blue')
axes[0].set_title(f'常態分布 (p={p_normal:.4f})')
axes[0].set_ylabel('頻率')

axes[1].hist(uniform_data, bins=50, alpha=0.7, color='green')
axes[1].set_title(f'均勻分布 (p={p_uniform:.4f})')

axes[2].hist(skewed_data, bins=50, alpha=0.7, color='red')
axes[2].set_title(f'右偏分布 (p={p_skewed:.4f})')

plt.tight_layout()
plt.show()

# 2️⃣ t檢定 - 比較平均值的差異
print("\n" + "="*50)
print("T檢定範例")
print("="*50)

# 情境：比較兩組學生的考試成績
group_A = np.random.normal(75, 8, 30)  # A組：平均75分
group_B = np.random.normal(80, 8, 30)  # B組：平均80分

# 獨立樣本 t檢定
t_stat, p_value = stats.ttest_ind(group_A, group_B)

print(f"A組平均分數：{np.mean(group_A):.2f}")
print(f"B組平均分數：{np.mean(group_B):.2f}")
print(f"平均分數差異：{np.mean(group_B) - np.mean(group_A):.2f}")
print(f"\n獨立樣本t檢定結果：")
print(f"t統計量：{t_stat:.4f}")
print(f"p值：{p_value:.6f}")

if p_value < 0.05:
    print("✅ 結論：兩組成績有顯著差異（p < 0.05）")
else:
    print("❌ 結論：兩組成績沒有顯著差異（p ≥ 0.05）")

# 配對樣本 t檢定（同一群人的前後測）
before_training = np.random.normal(70, 10, 25)
after_training = before_training + np.random.normal(5, 3, 25)  # 訓練後提升

paired_t_stat, paired_p_value = stats.ttest_rel(before_training, after_training)

print(f"\n=== 配對樣本t檢定（訓練前後比較）===")
print(f"訓練前平均：{np.mean(before_training):.2f}")
print(f"訓練後平均：{np.mean(after_training):.2f}")
print(f"t統計量：{paired_t_stat:.4f}")
print(f"p值：{paired_p_value:.6f}")

if paired_p_value < 0.05:
    print("✅ 結論：訓練有顯著效果（p < 0.05）")
else:
    print("❌ 結論：訓練沒有顯著效果（p ≥ 0.05）")

# 3️⃣ 變異數分析（ANOVA）- 比較多個群組
print(f"\n=== 單因子變異數分析（比較多個群組）===")

# 情境：比較三種教學方法的效果
method_A = np.random.normal(75, 8, 20)  # 傳統教學
method_B = np.random.normal(80, 8, 20)  # 線上教學
method_C = np.random.normal(82, 8, 20)  # 混合教學

f_stat, anova_p_value = stats.f_oneway(method_A, method_B, method_C)

print(f"傳統教學平均：{np.mean(method_A):.2f}")
print(f"線上教學平均：{np.mean(method_B):.2f}")
print(f"混合教學平均：{np.mean(method_C):.2f}")
print(f"\nANOVA檢定結果：")
print(f"F統計量：{f_stat:.4f}")
print(f"p值：{anova_p_value:.6f}")

if anova_p_value < 0.05:
    print("✅ 結論：至少有一種教學方法效果顯著不同（p < 0.05）")
else:
    print("❌ 結論：三種教學方法效果沒有顯著差異（p ≥ 0.05）")

# 4️⃣ 卡方檢定 - 分析類別變數的關聯
print(f"\n=== 卡方檢定（類別變數獨立性）===")

# 情境：性別與產品偏好是否有關聯？
# 建立列聯表
observed = np.array([
    [20, 30, 10],  # 男性對產品A、B、C的偏好
    [15, 25, 20]   # 女性對產品A、B、C的偏好
])

chi2_stat, chi2_p_value, dof, expected = stats.chi2_contingency(observed)

print("觀察頻率表（性別 vs 產品偏好）：")
print("       產品A  產品B  產品C")
print(f"男性     {observed[0,0]}     {observed[0,1]}     {observed[0,2]}")
print(f"女性     {observed[1,0]}     {observed[1,1]}     {observed[1,2]}")

print(f"\n卡方檢定結果：")
print(f"卡方統計量：{chi2_stat:.4f}")
print(f"自由度：{dof}")
print(f"p值：{chi2_p_value:.6f}")

if chi2_p_value < 0.05:
    print("✅ 結論：性別與產品偏好有顯著關聯（p < 0.05）")
else:
    print("❌ 結論：性別與產品偏好沒有顯著關聯（p ≥ 0.05）")

# 5️⃣ 相關性分析 - 測量變數間的線性關係
print(f"\n=== 相關性分析 ===")

# 生成有相關性的資料
x = np.random.normal(0, 1, 100)
y_strong = 0.8 * x + np.random.normal(0, 0.5, 100)    # 強正相關
y_weak = 0.3 * x + np.random.normal(0, 1, 100)       # 弱正相關

# Pearson 相關係數（線性相關）
pearson_strong, p_pearson_strong = stats.pearsonr(x, y_strong)
pearson_weak, p_pearson_weak = stats.pearsonr(x, y_weak)

# Spearman 相關係數（單調相關，對異常值較穩健）
spearman_strong, p_spearman_strong = stats.spearmanr(x, y_strong)
spearman_weak, p_spearman_weak = stats.spearmanr(x, y_weak)

print("強相關資料：")
print(f"  Pearson相關係數：{pearson_strong:.4f} (p={p_pearson_strong:.6f})")
print(f"  Spearman相關係數：{spearman_strong:.4f} (p={p_spearman_strong:.6f})")

print("弱相關資料：")
print(f"  Pearson相關係數：{pearson_weak:.4f} (p={p_pearson_weak:.6f})")
print(f"  Spearman相關係數：{spearman_weak:.4f} (p={p_spearman_weak:.6f})")

# 💡 相關係數解讀指南：
print(f"\n💡 相關係數解讀：")
print("  |r| < 0.3   ：弱相關")
print("  0.3 ≤ |r| < 0.7：中等相關") 
print("  |r| ≥ 0.7   ：強相關")

# 視覺化相關性
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.scatter(x, y_strong, alpha=0.6)
ax1.set_title(f'強相關 (r={pearson_strong:.3f})')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.grid(True, alpha=0.3)

ax2.scatter(x, y_weak, alpha=0.6)
ax2.set_title(f'弱相關 (r={pearson_weak:.3f})')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 💡 假設檢定小結：
print(f"\n💡 假設檢定使用指南：")
print("📊 常態性檢定：決定使用參數或非參數檢定")
print("📊 t檢定：比較1-2個群組的平均值")
print("📊 ANOVA：比較3個以上群組的平均值")
print("📊 卡方檢定：分析類別變數的關聯性")
print("📊 相關分析：測量連續變數的線性關係")
```

### 迴歸分析
```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 線性迴歸
model = LinearRegression()
model.fit(X, y)

# 預測
y_pred = model.predict(X_test)

# 模型評估
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# 模型係數
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
})

### (四) 迴歸分析進階詳解

**什麼是迴歸分析？**
- 🎯 **預測工具**：根據已知變數預測未知結果
- 📈 **關係建模**：量化變數間的影響關係
- 💰 **商業價值**：銷量預測、風險評估、定價策略
- 🔬 **科學研究**：驗證理論、探索因果關係

**迴歸分析的類型與應用**

| 類型 | 適用情況 | 商業應用範例 | 技術特點 |
|------|----------|--------------|----------|
| 簡單線性迴歸 | 一對一變數關係 | 廣告支出 → 銷量 | 易於解釋 |
| 多元線性迴歸 | 多因素影響 | 房屋特徵 → 價格 | 控制其他變數 |
| 邏輯迴歸 | 二元分類 | 客戶流失預測 | 機率輸出 |
| 多項式迴歸 | 非線性關係 | 溫度與電力需求 | 處理曲線關係 |

```python
# 📊 完整迴歸分析實例：房價預測

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# 1️⃣ 建立模擬房屋資料
np.random.seed(42)
n_houses = 500

house_data = pd.DataFrame({
    'size': np.random.normal(1500, 400, n_houses),           # 房屋面積（平方英尺）
    'bedrooms': np.random.randint(1, 6, n_houses),           # 臥室數量
    'bathrooms': np.random.randint(1, 4, n_houses),          # 浴室數量
    'age': np.random.randint(0, 30, n_houses),               # 房齡（年）
    'location_score': np.random.uniform(1, 10, n_houses),    # 位置評分（1-10）
    'garage': np.random.choice([0, 1, 2], n_houses)          # 車庫數量
})

# 確保資料在合理範圍內
house_data['size'] = np.clip(house_data['size'], 800, 3000)

# 2️⃣ 生成房價（基於多個因素的真實關係）
house_data['price'] = (
    150 * house_data['size'] +                    # 面積：每平方英尺$150
    8000 * house_data['bedrooms'] +               # 臥室：每間$8000
    5000 * house_data['bathrooms'] +              # 浴室：每間$5000
    -800 * house_data['age'] +                    # 房齡：每年貶值$800
    3000 * house_data['location_score'] +         # 位置：每分$3000
    10000 * house_data['garage'] +                # 車庫：每個$10000
    50000 +                                       # 基礎價格
    np.random.normal(0, 15000, n_houses)          # 隨機誤差
)

# 確保房價在合理範圍
house_data['price'] = np.clip(house_data['price'], 100000, 800000)

print("📊 房屋資料概覽：")
print(house_data.describe())

# 3️⃣ 資料準備和分割
features = ['size', 'bedrooms', 'bathrooms', 'age', 'location_score', 'garage']
X = house_data[features]
y = house_data['price']

# 分割資料（70%訓練，30%測試）
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"\n📈 資料分割結果：")
print(f"訓練集：{len(X_train)} 筆資料")
print(f"測試集：{len(X_test)} 筆資料")

# 4️⃣ 建立和訓練模型
model = LinearRegression()
model.fit(X_train, y_train)

# 5️⃣ 模型預測和評估
y_pred_train = model.predict(X_train)  # 訓練集預測
y_pred_test = model.predict(X_test)    # 測試集預測

# 計算評估指標
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
test_mae = mean_absolute_error(y_test, y_pred_test)

print(f"\n📊 模型效能評估：")
print(f"訓練集 R²：{train_r2:.4f}")
print(f"測試集 R²：{test_r2:.4f}")
print(f"測試集 RMSE：${test_rmse:,.0f}")
print(f"測試集 MAE：${test_mae:,.0f}")

# 檢查過度配適
if train_r2 - test_r2 > 0.1:
    print("⚠️ 警告：可能存在過度配適")
else:
    print("✅ 模型泛化能力良好")

# 6️⃣ 特徵重要性分析
feature_importance = pd.DataFrame({
    'Feature': features,
    'Coefficient': model.coef_,
    'Abs_Coefficient': np.abs(model.coef_),
    'Impact_Percentage': np.abs(model.coef_) / np.sum(np.abs(model.coef_)) * 100
}).sort_values('Abs_Coefficient', ascending=False)

print(f"\n🔍 特徵重要性排名：")
print(feature_importance[['Feature', 'Coefficient', 'Impact_Percentage']])

# 7️⃣ 商業解釋
print(f"\n💼 商業意義解釋：")
for i, row in feature_importance.iterrows():
    feature = row['Feature']
    coef = row['Coefficient']
    
    if feature == 'size':
        print(f"📏 房屋面積：每增加1平方英尺，房價增加 ${coef:.0f}")
    elif feature == 'bedrooms':
        print(f"🛏️ 臥室數量：每增加1間臥室，房價增加 ${coef:,.0f}")
    elif feature == 'bathrooms':
        print(f"🚿 浴室數量：每增加1間浴室，房價增加 ${coef:,.0f}")
    elif feature == 'age':
        print(f"📅 房屋年齡：每增加1年，房價減少 ${abs(coef):,.0f}")
    elif feature == 'location_score':
        print(f"📍 位置評分：每提升1分，房價增加 ${coef:,.0f}")
    elif feature == 'garage':
        print(f"🚗 車庫數量：每增加1個車庫，房價增加 ${coef:,.0f}")

# 8️⃣ 實際預測示例
print(f"\n🏠 實際預測示例：")
sample_house = {
    'size': 2000,
    'bedrooms': 3,
    'bathrooms': 2,
    'age': 10,
    'location_score': 7.5,
    'garage': 2
}

sample_df = pd.DataFrame([sample_house])
predicted_price = model.predict(sample_df)[0]

print(f"房屋特徵：")
for key, value in sample_house.items():
    print(f"  {key}: {value}")
print(f"預測房價：${predicted_price:,.0f}")

# 9️⃣ 視覺化分析
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 預測 vs 實際
axes[0,0].scatter(y_test, y_pred_test, alpha=0.6)
axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0,0].set_xlabel('實際房價')
axes[0,0].set_ylabel('預測房價')
axes[0,0].set_title(f'預測 vs 實際 (R² = {test_r2:.3f})')

# 殘差圖
residuals = y_test - y_pred_test
axes[0,1].scatter(y_pred_test, residuals, alpha=0.6)
axes[0,1].axhline(y=0, color='r', linestyle='--')
axes[0,1].set_xlabel('預測房價')
axes[0,1].set_ylabel('殘差')
axes[0,1].set_title('殘差分析')

# 特徵重要性
axes[1,0].barh(feature_importance['Feature'], feature_importance['Abs_Coefficient'])
axes[1,0].set_xlabel('係數絕對值')
axes[1,0].set_title('特徵重要性')

# 預測誤差分布
axes[1,1].hist(residuals, bins=30, alpha=0.7)
axes[1,1].set_xlabel('殘差')
axes[1,1].set_ylabel('頻率')
axes[1,1].set_title('殘差分布')

plt.tight_layout()
plt.show()

# 🔟 模型診斷檢查
print(f"\n🔍 模型診斷檢查：")

# 檢查殘差的正態性
from scipy import stats
_, normality_p = stats.shapiro(residuals[:100])  # 限制樣本數
print(f"殘差正態性檢定 p值：{normality_p:.4f}")
if normality_p > 0.05:
    print("✅ 殘差符合正態分布")
else:
    print("⚠️ 殘差不符合正態分布")

# 檢查線性關係
correlation_matrix = X.corr()
print(f"\n特徵間相關性（檢查多重共線性）：")
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr = correlation_matrix.iloc[i, j]
        if abs(corr) > 0.7:
            high_corr_pairs.append((correlation_matrix.columns[i], 
                                  correlation_matrix.columns[j], corr))

if high_corr_pairs:
    print("⚠️ 發現高相關特徵對：")
    for pair in high_corr_pairs:
        print(f"  {pair[0]} - {pair[1]}: {pair[2]:.3f}")
else:
    print("✅ 無嚴重多重共線性問題")
```

## 四、學習進階指南

### (一) 學習階段規劃

**🎯 初學者路徑（4-6週）**

**第1週：環境設置與基礎工具**
- 🛠️ **環境準備**：安裝 Anaconda、Jupyter Notebook
- 🐼 **Pandas 入門**：DataFrame 操作、資料讀取
- 📊 **基礎視覺化**：Matplotlib 基本圖表
- 💡 **實作**：分析一個簡單的 CSV 檔案

**第2週：資料探索與清理**
- 🔍 **資料探索**：`describe()`, `info()`, 缺失值檢查
- 🧹 **資料清理**：處理缺失值、重複值、異常值
- 📈 **描述統計**：平均值、中位數、標準差的計算與解釋
- 💡 **實作**：清理一個「髒」資料集

**第3週：進階資料處理**
- 🔄 **資料重塑**：`pivot()`, `melt()`, `groupby()`
- 🔗 **資料合併**：不同類型的 join 操作
- 🎨 **進階視覺化**：Seaborn 統計圖表
- 💡 **實作**：多表格資料整合分析

**第4週：統計分析基礎**
- 📊 **假設檢定**：t檢定、ANOVA 的實際應用
- 📈 **相關分析**：Pearson、Spearman 相關係數
- 🌐 **互動視覺化**：Plotly 基礎圖表
- 💡 **實作**：A/B 測試效果分析

**第5-6週：迴歸分析與專案**
- 🎯 **迴歸建模**：線性迴歸、模型評估
- 🔍 **模型診斷**：殘差分析、假設檢驗
- 📋 **完整專案**：從資料收集到結論報告
- 💡 **實作**：預測模型建立與驗證

### (二) 實戰專案推薦

**🥉 初級專案：零售銷售分析**
```
專案目標：分析超市半年銷售資料
資料規模：1000-5000 筆交易記錄
技能重點：Pandas 操作、基礎統計、Matplotlib
預期產出：
- 📊 月度銷售趨勢圖
- 🏆 TOP 10 產品排行榜
- 📈 季節性分析報告
- 💰 營收來源分析
學習時間：1-2 週
```

**🥈 中級專案：客戶行為分析**
```
專案目標：電商網站用戶行為分析
資料規模：10000+ 用戶行為記錄
技能重點：資料清理、統計檢定、Seaborn
預期產出：
- 👥 客戶分群分析
- 🔄 流失客戶識別
- 📱 購買路徑分析  
- 🎯 個人化推薦策略
學習時間：3-4 週
```

**🥇 高級專案：營運績效監控系統**
```
專案目標：建立即時營運監控儀表板
資料規模：多數據源整合
技能重點：完整資料科學流程、Plotly、預測建模
預期產出：
- 📊 即時KPI監控面板
- 🔮 銷量預測模型
- 🚨 異常檢測系統
- 📱 行動端友善介面
學習時間：6-8 週
```

### (三) 進階技能發展

**🔬 統計學進階**
- **貝葉斯統計**：不確定性量化、先驗知識整合
- **時間序列分析**：趨勢分析、季節性分解、ARIMA模型
- **存活分析**：客戶生命週期、流失時間預測
- **因果推論**：A/B測試設計、因果關係識別

**🤖 機器學習入門**
- **監督學習**：決策樹、隨機森林、支援向量機
- **非監督學習**：K-means聚類、PCA降維
- **模型選擇**：交叉驗證、網格搜尋、模型比較
- **特徵工程**：特徵選擇、創造、轉換

**📊 進階視覺化**
- **Dashboard 開發**：Streamlit、Dash 互動應用
- **地理資料視覺化**：Folium 地圖、Geopandas
- **大數據視覺化**：Datashader、Bokeh
- **商業智慧工具**：Tableau、Power BI 整合

### (四) 學習資源與工具

**📚 推薦書籍**
- **入門級**：《Python資料科學手冊》- Jake VanderPlas
- **進階級**：《統計學習導論》- Gareth James
- **實務級**：《數據科學實戰》- Joel Grus

**💻 線上平台**
- **免費資源**：Kaggle Learn、YouTube 教學頻道
- **付費課程**：Coursera、edX、DataCamp
- **實作練習**：Kaggle Competitions、GitHub Projects

**🛠️ 開發工具**
- **IDE環境**：Jupyter Notebook、VS Code、PyCharm
- **雲端平台**：Google Colab、Kaggle Kernels、AWS SageMaker
- **版本控制**：Git、GitHub、資料版本控制 DVC

**🌐 資料來源**
- **公開資料集**：Kaggle、UCI ML Repository、政府開放資料
- **API資料**：Twitter API、股票市場 API、天氣資料 API
- **網路爬蟲**：BeautifulSoup、Scrapy、Selenium

### (五) 職涯發展方向

**📈 資料分析師路徑**
- **初級**：資料清理、基礎分析、報表製作
- **中級**：商業洞察、預測模型、A/B測試
- **高級**：策略分析、產品分析、營運優化

**🔬 資料科學家路徑**
- **技能需求**：統計學、機器學習、程式設計
- **工作內容**：演算法開發、模型部署、研究創新
- **發展方向**：深度學習、自然語言處理、電腦視覺

**💼 商業分析師路徑**
- **核心能力**：商業理解、溝通技巧、策略思維
- **工作重點**：市場分析、競爭情報、投資評估
- **價值創造**：商業決策支援、流程優化、風險管理

記住，**實作是最好的學習方式**！🚀 從小專案開始，逐步建立信心和技能，每個專案都是向專業資料分析師邁進的一步。
